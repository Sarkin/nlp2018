{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW DNN 6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Sarkin/nlp2018/blob/master/HW_DNN_6.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "dF1fio53UKN6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.cuda import FloatTensor, LongTensor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q3lN5pl5Stpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Свёрточные нейронные сети"
      ]
    },
    {
      "metadata": {
        "id": "rRhcCJcAS4S5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напомню, свертки - это то, с чего начался хайп нейронных сетей в районе 2012-ого.\n",
        "\n",
        "Работают они примерно так:  \n",
        "![Conv example](http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)   \n",
        "From [Feature extraction using convolution](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution).\n",
        "\n",
        "Формально - учатся наборы фильтров, каждый из которых скалярно умножается на элементы матрицы признаков. На картинке выше исходная матрица сворачивается с фильтром\n",
        "$$\n",
        " \\begin{pmatrix}\n",
        "  1 & 0 & 1 \\\\\n",
        "  0 & 1 & 0 \\\\\n",
        "  1 & 0 & 1\n",
        " \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Но нужно не забывать, что свертки обычно имеют ещё такую размерность, как число каналов. Например, картинки имеют обычно три канала: RGB.  \n",
        "Наглядно демонстрируется как выглядят при этом фильтры [здесь](http://cs231n.github.io/convolutional-networks/#conv).\n",
        "\n",
        "После сверток обычно следуют pooling-слои. Они помогают уменьшить размерность тензора, с которым приходится работать. Самым частым является max-pooling:  \n",
        "![maxpooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg =x300)  \n",
        "From [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/#pool)"
      ]
    },
    {
      "metadata": {
        "id": "x-M3lCE1ealB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Character-Level Convolutions\n",
        "Мы говорим про свертки для работы с текстами. Совсем не очевидно, что они вообще должны помочь в работе с текстами. То есть в изображениях они отлавливают некоторые локальные особенности, такие как:\n",
        "![weights](https://i.stack.imgur.com/Hl2H6.png)\n",
        "\n",
        "Для текстов свертки работают как n-граммные детекторы (примерно). Посмотрите на пример символьной сверточной сети:\n",
        "\n",
        "![text-convs](https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png =x500)  \n",
        "From [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)\n",
        "\n",
        "*Сколько учится фильтров на данном примере?*\n",
        "\n",
        "На картинке показано, как из слова извлекаются 2, 3 и 4-граммы. Например, желтые - это триграммы. Желтый фильтр прикладывают ко всем триграммам в слове, а потом с помощью global max-pooling извлекают наиболее сильный сигнал.\n",
        "\n",
        "Что это значит, если конкретнее?\n",
        "\n",
        "Каждый символ отображается с помощью эмбеддингов в некоторый вектор. А их последовательности - в конкатенации эмбеддингов.  \n",
        "Например, \"abs\" $\\to [v_a; v_b; v_s] \\in \\mathbb{R}^{3 d}$, где $d$ - размерность эмбеддинга. Желтый фильтр $f_k$ имеет такую же размерность $3d$.  \n",
        "Его прикладывание - это скалярное произведение $\\left([v_a; v_b; v_s] \\odot f_k \\right) \\in \\mathbb R$ (один из желтых квадратиков в feature map для данного фильтра).\n",
        "\n",
        "Max-pooling выбирает $max_i \\left( [v_{i-1}; v_{i}; v_{i+1}] \\odot f_k \\right)$, где $i$ пробегается по всем индексам слова от 1 до $|w| - 1$ (либо по большему диапазону, если есть padding'и).   \n",
        "Этот максимум соответствует той триграмме, которая наиболее близка к фильтру по косинусному расстоянию.\n",
        "\n",
        "В результате в векторе после max-pooling'а закодирована информация о том, какие из n-грамм встретились в слове: если встретилась близкая к нашему $f_k$ триграмма, то в $k$-той позиции вектора будет стоять большое значение, иначе - маленькое.\n",
        "\n",
        "А учим мы как раз фильтры. То есть сеть должна научиться определять, какие из n-грамм значимы, а какие - нет."
      ]
    },
    {
      "metadata": {
        "id": "9J6KBv-cniLw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Классификация слов\n",
        "\n",
        "Будем учиться предсказывать, является ли слово фамилией.\n",
        "\n",
        "Скачаем данные."
      ]
    },
    {
      "metadata": {
        "id": "x6NEERMkR__8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -qq -O surnames.txt https://share.abbyy.com/index.php/s/mt5r9vEZo70sfIS/download"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFUvFUgHTs47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('surnames.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    data = [line.strip().split('\\t')[0] for line in lines]\n",
        "    labels = [int(line.strip().split('\\t')[1]) for line in lines]\n",
        "    del lines\n",
        "    \n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfUIgqMznrXB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Данные, как видно, грязноваты:"
      ]
    },
    {
      "metadata": {
        "id": "YN1XYen8UauD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "9b29aeff-e5b3-47dd-c188-3ecf3e7454ee"
      },
      "cell_type": "code",
      "source": [
        "list(zip(X_train, y_train))[:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('аминов', 1),\n",
              " ('переточек', 0),\n",
              " ('Оманом', 0),\n",
              " ('Говядину', 0),\n",
              " ('дарвинистам', 0),\n",
              " ('зёрнышком', 0),\n",
              " ('Ролинсе', 1),\n",
              " ('гостиница', 0),\n",
              " ('Суфизм', 0),\n",
              " ('ядовитостью', 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "LJJtiGMK1ATy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Начнем с бейзлайна - логистической регрессии на n-граммах символов.\n",
        "\n",
        "**Задание** Сделать классификацию с LogisticRegression моделью. Посчитать F1-меру."
      ]
    },
    {
      "metadata": {
        "id": "8COAoh7b0TXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3,3), lowercase=False)\n",
        "\n",
        "X_train_sparse = vectorizer.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qvPfpH2Yi2bd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_sparse = vectorizer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GbHKoxuhJS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "820be6d9-7269-48ce-e8b1-54bc555bfe83"
      },
      "cell_type": "code",
      "source": [
        "print(X_train_sparse.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(67943, 17425)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g5PUS31BjD0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "ddef238d-a8bc-4142-d680-077e457f48bf"
      },
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_sparse, y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
              "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
              "          verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "PbzRUGldjPA5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86d0ad28-67a3-48e7-f620-cbe864d662fd"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, precision_score, recall_score\n",
        "\n",
        "y_pred = lr.predict(X_test_sparse)\n",
        "\n",
        "print(\"Accuracy: {} F1 score {}\".format(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9232631107126849 F1 score 0.5178370259106271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h7ZVbNOGoV5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Как всегда, сконвертируем их для начала:"
      ]
    },
    {
      "metadata": {
        "id": "DEtZ6g78Wtj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e208779c-c18a-49aa-b8f8-b7600d3939b3"
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter \n",
        "    \n",
        "def find_max_len(counter, threshold):\n",
        "    sum_count = sum(counter.values())\n",
        "    cum_count = 0\n",
        "    for i in range(max(counter)):\n",
        "        cum_count += counter[i]\n",
        "        if cum_count > sum_count * threshold:\n",
        "            return i\n",
        "    return max(counter)\n",
        "\n",
        "word_len_counter = Counter()\n",
        "for word in X_train:\n",
        "    word_len_counter[len(word)] += 1\n",
        "    \n",
        "threshold = 0.99\n",
        "MAX_WORD_LEN = find_max_len(word_len_counter, threshold)\n",
        "\n",
        "print('Max word len for {:.0%} of words is {}'.format(threshold, MAX_WORD_LEN))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max word len for 99% of words is 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YyMoPEXGVs3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "257dc62f-15a1-48d3-8be5-994d3fcc9842"
      },
      "cell_type": "code",
      "source": [
        "chars = set()\n",
        "for word in X_train:\n",
        "    chars.update(word)\n",
        "\n",
        "char_index = {c : i + 1 for i, c in enumerate(chars)}\n",
        "char_index[''] = 0\n",
        "\n",
        "def get_char_index(char, char_index):\n",
        "    return char_index[char] if char in char_index else len(char_index)\n",
        "  \n",
        "print(char_index)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'п': 1, 'ч': 2, '-': 3, '/': 4, 'М': 5, '«': 6, 'ф': 7, ' ': 8, 'у': 9, '2': 10, '’': 11, 'Д': 12, 'Ы': 13, '.': 14, '7': 15, 'м': 16, 'Щ': 17, 'Б': 18, 'Ь': 19, 'т': 20, 'к': 21, 'щ': 22, 'в': 23, 'о': 24, 'а': 25, 'ы': 26, 'г': 27, 'В': 28, 'Е': 29, 'х': 30, 'е': 31, '3': 32, 'ъ': 33, 'Л': 34, 'д': 35, 'Ё': 36, \"'\": 37, 'ё': 38, 'н': 39, 'ó': 40, 'Р': 41, 'ю': 42, 'У': 43, 'й': 44, 'Ж': 45, 'К': 46, 'Х': 47, 'ж': 48, 'Я': 49, 'Т': 50, 'А': 51, 'Ш': 52, 'з': 53, 'Ъ': 54, 'б': 55, 'И': 56, 'З': 57, 'П': 58, 'ц': 59, 'ѐ': 60, 'Ф': 61, 'и': 62, 'р': 63, 'Ч': 64, 'Г': 65, 'Ю': 66, 'Ó': 67, '»': 68, '·': 69, 'я': 70, 'Н': 71, 'л': 72, 'ь': 73, 'Ц': 74, 'с': 75, 'ш': 76, 'Й': 77, 'э': 78, 'Э': 79, 'О': 80, 'С': 81, '': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qETmYKm8W_TX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_data(data, max_word_len, char_index):\n",
        "    X = np.zeros((len(data), max_word_len))\n",
        "    for i, word in enumerate(data):\n",
        "        word = word[-max_word_len:]\n",
        "        X[i, :len(word)] = [get_char_index(symb, char_index) for symb in word]\n",
        "        \n",
        "    return LongTensor(X)\n",
        "  \n",
        "X_train = convert_data(X_train, MAX_WORD_LEN, char_index)\n",
        "X_test = convert_data(X_test, MAX_WORD_LEN, char_index)\n",
        "\n",
        "y_train = FloatTensor(y_train)\n",
        "y_test = FloatTensor(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VufrP006Vk-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def iterate_batches(dataset, batch_size):\n",
        "    X, y = dataset\n",
        "    num_samples = X.shape[0]\n",
        "\n",
        "    indices = np.arange(num_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        end = min(start + batch_size, num_samples)\n",
        "        \n",
        "        batch_idx = indices[start:end]\n",
        "        \n",
        "        yield Variable(X[batch_idx, ]), Variable(y[batch_idx, ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkXlKB7lobYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь построим свёрточную модель.\n",
        "\n",
        "Типичным является блок:\n",
        "```python\n",
        "nn.Conv*d(in_channels=N, out_channels=M, kernel_size=K1, padding=0)\n",
        "F.relu\n",
        "nn.MaxPool*d(kernel_size=K2)\n",
        "```\n",
        "\n",
        "Пусть она будет строить триграммы - то есть применять фильтры на 3 символа.\n",
        "\n",
        "Какие нам нужны размерности?"
      ]
    },
    {
      "metadata": {
        "id": "v2T4AorsZ530",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, filters_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.char_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.cnn = nn.Conv2d(in_channels=1, out_channels=filters_count, kernel_size=(3, emb_dim))\n",
        "        self.pool = nn.MaxPool1d(MAX_WORD_LEN - 2)\n",
        "        self.out = nn.Linear(filters_count, 1)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        '''\n",
        "        inp.size() = (batch_size, max_word_len)\n",
        "        out.size() = (batch_size,)\n",
        "        '''\n",
        "        emb = self.char_emb(inp).unsqueeze(1)\n",
        "        cnn_out = F.relu(self.cnn(emb)).squeeze()\n",
        "        pool_out = self.pool(cnn_out).squeeze()\n",
        "        return self.out(pool_out).squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JqiBVpBwqmb_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В данной задаче несбалансированные классы, поэтому хочется мерять $F_1$-меру.\n",
        "\n",
        "Напомню:\n",
        "\n",
        "![precision-recall](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png =x600)  \n",
        "From [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
        "\n",
        "$$\\text{precision} = \\frac{tp}{tp + fp}.$$\n",
        "$$\\text{recall} = \\frac{tp}{tp + fn}.$$\n",
        "$$\\text{F}_1 = 2\\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}.$$"
      ]
    },
    {
      "metadata": {
        "id": "NJOnH2-rqmJu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "713be35b-5fca-4551-aa29-beabe96f66dd"
      },
      "cell_type": "code",
      "source": [
        "model = ConvClassifier(len(char_index) + 1, 24, 128).cuda()\n",
        "\n",
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 32))\n",
        "\n",
        "#<calculate precision, recall and F1-score>\n",
        "logit = model(X_batch)\n",
        "preds = (logit.sigmoid() > 0.5)\n",
        "precision_recall_fscore_support(y_batch.data, preds.data)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.875, 0.   ]),\n",
              " array([1., 0.]),\n",
              " array([0.93333333, 0.        ]),\n",
              " array([28,  4]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "metadata": {
        "id": "MsCtTJucVjMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
        "    epoch_loss = 0\n",
        "    epoch_tp = 0\n",
        "    epoch_tpfn = 0\n",
        "    epoch_tpfp = 0\n",
        "    \n",
        "    model.train(not optimizer is None)\n",
        "    \n",
        "    batchs_count = math.ceil(data[0].shape[0] / batch_size)\n",
        "    \n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "        logits = model(X_batch)\n",
        "        \n",
        "        loss = criterion(logits, y_batch)\n",
        "        epoch_loss += loss.data[0]\n",
        "        \n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        preds = (logits.sigmoid() > 0.5).long()\n",
        "        epoch_tp += (preds * y_batch.long()).sum()\n",
        "        epoch_tpfn += (y_batch.long() * (1 - preds)).sum()\n",
        "        epoch_tpfp += ((1 - y_batch.long()) * preds).sum()\n",
        "    \n",
        "        \n",
        "        precision = precision_score(y_batch.data, preds.data)\n",
        "        recall = recall_score(y_batch.data, preds.data)\n",
        "        f1 = f1_score(y_batch.data, preds.data)\n",
        "      \n",
        "        print('\\r[{} / {}]: Loss = {:.5f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'.format(\n",
        "              i, batchs_count, loss.data[0], precision, recall, f1), end='')\n",
        "        \n",
        "    \n",
        "    epoch_tp = epoch_tp.data[0]\n",
        "    epoch_tpfn = epoch_tpfn.data[0]\n",
        "    epoch_tpfp = epoch_tpfp.data[0]\n",
        "    precision = epoch_tp / (epoch_tp + epoch_tpfp)\n",
        "    recall = epoch_tp / (epoch_tp + epoch_tpfn)\n",
        "    f1 = 2 * precision *  recall / (precision + recall)\n",
        "    \n",
        "    return epoch_loss / batchs_count, recall, precision, f1\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
        "        batch_size=32, val_data=None, val_batch_size=None):\n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_recall, train_precision, train_f1 = \\\n",
        "            do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
        "        \n",
        "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.5f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
        "        if not val_data is None:\n",
        "            val_loss, val_recall, val_precision, val_f1 = \\\n",
        "                do_epoch(model, criterion, val_data, val_batch_size, None)\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            output_info += ', Val Loss = {:.5f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
        "                                     train_loss, train_recall, train_precision, train_f1,\n",
        "                                     val_loss, val_recall, val_precision, val_f1))\n",
        "        else:\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlq63hAXh0Gr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1843
        },
        "outputId": "adbe0939-55fc-4a49-d285-9baa66cb0516"
      },
      "cell_type": "code",
      "source": [
        "model = ConvClassifier(len(char_index) + 1, 24, 256).cuda()\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().cuda()\n",
        "\n",
        "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad])\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=100, \n",
        "    batch_size=512, val_data=(X_test, y_test), val_batch_size=1024)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[0 / 133]: Loss = 0.47262, Precision = 100.00%, Recall = 1.92%, F1 = 3.77%\r[1 / 133]: Loss = 0.41025, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[2 / 133]: Loss = 0.35779, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[3 / 133]: Loss = 0.34898, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[4 / 133]: Loss = 0.38162, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[5 / 133]: Loss = 0.32265, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 100, Epoch Time = 4.77s: Train Loss = 0.29604, Precision = 1.37%, Recall = 62.42%, F1 = 2.69%, Val Loss = 0.26623, Precision = 5.03%, Recall = 66.92%, F1 = 9.36%\n",
            "Epoch 2 / 100, Epoch Time = 4.66s: Train Loss = 0.24874, Precision = 16.23%, Recall = 68.18%, F1 = 26.22%, Val Loss = 0.24182, Precision = 28.78%, Recall = 62.59%, F1 = 39.43%\n",
            "Epoch 3 / 100, Epoch Time = 4.73s: Train Loss = 0.22903, Precision = 28.81%, Recall = 69.87%, F1 = 40.80%, Val Loss = 0.22896, Precision = 29.07%, Recall = 68.53%, F1 = 40.82%\n",
            "Epoch 4 / 100, Epoch Time = 4.73s: Train Loss = 0.21786, Precision = 34.37%, Recall = 71.46%, F1 = 46.42%, Val Loss = 0.22240, Precision = 36.12%, Recall = 68.40%, F1 = 47.28%\n",
            "Epoch 5 / 100, Epoch Time = 4.70s: Train Loss = 0.20974, Precision = 37.76%, Recall = 72.94%, F1 = 49.76%, Val Loss = 0.22013, Precision = 39.64%, Recall = 67.20%, F1 = 49.87%\n",
            "Epoch 6 / 100, Epoch Time = 4.68s: Train Loss = 0.20344, Precision = 40.43%, Recall = 73.60%, F1 = 52.19%, Val Loss = 0.21481, Precision = 42.53%, Recall = 67.48%, F1 = 52.17%\n",
            "Epoch 7 / 100, Epoch Time = 4.77s: Train Loss = 0.19873, Precision = 42.47%, Recall = 74.96%, F1 = 54.22%, Val Loss = 0.21330, Precision = 42.33%, Recall = 68.95%, F1 = 52.45%\n",
            "[56 / 133]: Loss = 0.15846, Precision = 73.33%, Recall = 44.00%, F1 = 55.00%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8 / 100, Epoch Time = 4.71s: Train Loss = 0.19328, Precision = 44.78%, Recall = 75.81%, F1 = 56.31%, Val Loss = 0.21127, Precision = 40.58%, Recall = 69.30%, F1 = 51.19%\n",
            "Epoch 9 / 100, Epoch Time = 4.69s: Train Loss = 0.18860, Precision = 46.56%, Recall = 76.71%, F1 = 57.95%, Val Loss = 0.21157, Precision = 35.95%, Recall = 71.60%, F1 = 47.87%\n",
            "Epoch 10 / 100, Epoch Time = 4.65s: Train Loss = 0.18439, Precision = 48.00%, Recall = 77.55%, F1 = 59.30%, Val Loss = 0.21071, Precision = 49.04%, Recall = 66.87%, F1 = 56.59%\n",
            "Epoch 11 / 100, Epoch Time = 4.67s: Train Loss = 0.18075, Precision = 49.56%, Recall = 78.24%, F1 = 60.68%, Val Loss = 0.20768, Precision = 43.93%, Recall = 69.96%, F1 = 53.97%\n",
            "Epoch 12 / 100, Epoch Time = 4.61s: Train Loss = 0.17659, Precision = 51.53%, Recall = 78.97%, F1 = 62.37%, Val Loss = 0.20759, Precision = 40.95%, Recall = 72.05%, F1 = 52.22%\n",
            "Epoch 13 / 100, Epoch Time = 4.75s: Train Loss = 0.17378, Precision = 51.52%, Recall = 79.18%, F1 = 62.42%, Val Loss = 0.20703, Precision = 50.21%, Recall = 67.37%, F1 = 57.54%\n",
            "Epoch 14 / 100, Epoch Time = 4.67s: Train Loss = 0.17092, Precision = 53.20%, Recall = 79.52%, F1 = 63.75%, Val Loss = 0.20495, Precision = 46.07%, Recall = 70.33%, F1 = 55.67%\n",
            "[58 / 133]: Loss = 0.15182, Precision = 78.38%, Recall = 60.42%, F1 = 68.24%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15 / 100, Epoch Time = 4.65s: Train Loss = 0.16722, Precision = 53.65%, Recall = 80.39%, F1 = 64.35%, Val Loss = 0.20705, Precision = 50.93%, Recall = 67.58%, F1 = 58.08%\n",
            "Epoch 16 / 100, Epoch Time = 4.72s: Train Loss = 0.16343, Precision = 55.26%, Recall = 80.99%, F1 = 65.70%, Val Loss = 0.20634, Precision = 46.04%, Recall = 70.69%, F1 = 55.76%\n",
            "Epoch 17 / 100, Epoch Time = 4.65s: Train Loss = 0.16115, Precision = 56.17%, Recall = 81.09%, F1 = 66.37%, Val Loss = 0.20559, Precision = 49.64%, Recall = 69.23%, F1 = 57.82%\n",
            "Epoch 18 / 100, Epoch Time = 4.62s: Train Loss = 0.15920, Precision = 56.13%, Recall = 81.11%, F1 = 66.35%, Val Loss = 0.20717, Precision = 48.76%, Recall = 70.00%, F1 = 57.48%\n",
            "Epoch 19 / 100, Epoch Time = 4.67s: Train Loss = 0.15495, Precision = 58.37%, Recall = 82.19%, F1 = 68.26%, Val Loss = 0.20486, Precision = 47.90%, Recall = 70.93%, F1 = 57.18%\n",
            "Epoch 20 / 100, Epoch Time = 4.74s: Train Loss = 0.15313, Precision = 58.83%, Recall = 81.98%, F1 = 68.50%, Val Loss = 0.20517, Precision = 49.90%, Recall = 70.43%, F1 = 58.41%\n",
            "Epoch 21 / 100, Epoch Time = 4.65s: Train Loss = 0.15002, Precision = 59.43%, Recall = 82.92%, F1 = 69.24%, Val Loss = 0.20779, Precision = 54.24%, Recall = 66.48%, F1 = 59.74%\n",
            "[54 / 133]: Loss = 0.15764, Precision = 94.29%, Recall = 53.23%, F1 = 68.04%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22 / 100, Epoch Time = 4.68s: Train Loss = 0.14809, Precision = 60.08%, Recall = 82.85%, F1 = 69.65%, Val Loss = 0.20657, Precision = 50.21%, Recall = 70.65%, F1 = 58.70%\n",
            "Epoch 23 / 100, Epoch Time = 4.67s: Train Loss = 0.14538, Precision = 60.48%, Recall = 83.86%, F1 = 70.28%, Val Loss = 0.20804, Precision = 54.33%, Recall = 67.17%, F1 = 60.07%\n",
            "Epoch 24 / 100, Epoch Time = 4.68s: Train Loss = 0.14297, Precision = 61.73%, Recall = 83.62%, F1 = 71.03%, Val Loss = 0.20980, Precision = 43.78%, Recall = 74.95%, F1 = 55.28%\n",
            "Epoch 25 / 100, Epoch Time = 4.72s: Train Loss = 0.14035, Precision = 62.08%, Recall = 83.97%, F1 = 71.39%, Val Loss = 0.20840, Precision = 47.36%, Recall = 73.32%, F1 = 57.54%\n",
            "Epoch 26 / 100, Epoch Time = 4.68s: Train Loss = 0.13792, Precision = 62.99%, Recall = 84.29%, F1 = 72.10%, Val Loss = 0.20782, Precision = 46.56%, Recall = 74.49%, F1 = 57.30%\n",
            "Epoch 27 / 100, Epoch Time = 4.72s: Train Loss = 0.13600, Precision = 63.58%, Recall = 84.38%, F1 = 72.52%, Val Loss = 0.20928, Precision = 46.98%, Recall = 73.52%, F1 = 57.33%\n",
            "Epoch 28 / 100, Epoch Time = 4.72s: Train Loss = 0.13344, Precision = 64.03%, Recall = 85.15%, F1 = 73.10%, Val Loss = 0.20858, Precision = 47.50%, Recall = 72.58%, F1 = 57.42%\n",
            "[56 / 133]: Loss = 0.12481, Precision = 89.09%, Recall = 74.24%, F1 = 80.99%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29 / 100, Epoch Time = 4.70s: Train Loss = 0.13241, Precision = 64.91%, Recall = 85.66%, F1 = 73.85%, Val Loss = 0.20996, Precision = 54.53%, Recall = 67.28%, F1 = 60.24%\n",
            "Epoch 30 / 100, Epoch Time = 4.77s: Train Loss = 0.12946, Precision = 65.70%, Recall = 85.37%, F1 = 74.25%, Val Loss = 0.20962, Precision = 48.24%, Recall = 72.54%, F1 = 57.95%\n",
            "Epoch 31 / 100, Epoch Time = 4.67s: Train Loss = 0.12758, Precision = 65.99%, Recall = 85.65%, F1 = 74.55%, Val Loss = 0.21389, Precision = 45.10%, Recall = 74.26%, F1 = 56.12%\n",
            "Epoch 32 / 100, Epoch Time = 4.71s: Train Loss = 0.12555, Precision = 66.98%, Recall = 85.58%, F1 = 75.15%, Val Loss = 0.21267, Precision = 53.39%, Recall = 66.31%, F1 = 59.15%\n",
            "Epoch 33 / 100, Epoch Time = 4.66s: Train Loss = 0.12537, Precision = 66.97%, Recall = 85.85%, F1 = 75.24%, Val Loss = 0.21156, Precision = 48.01%, Recall = 72.51%, F1 = 57.77%\n",
            "Epoch 34 / 100, Epoch Time = 4.73s: Train Loss = 0.12237, Precision = 67.19%, Recall = 86.75%, F1 = 75.73%, Val Loss = 0.21304, Precision = 54.47%, Recall = 65.36%, F1 = 59.42%\n",
            "Epoch 35 / 100, Epoch Time = 4.68s: Train Loss = 0.12099, Precision = 68.02%, Recall = 86.42%, F1 = 76.12%, Val Loss = 0.21486, Precision = 49.73%, Recall = 70.30%, F1 = 58.25%\n",
            "[59 / 133]: Loss = 0.13928, Precision = 97.22%, Recall = 58.33%, F1 = 72.92%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36 / 100, Epoch Time = 4.78s: Train Loss = 0.12157, Precision = 68.23%, Recall = 85.89%, F1 = 76.05%, Val Loss = 0.21698, Precision = 56.39%, Recall = 63.48%, F1 = 59.72%\n",
            "Epoch 37 / 100, Epoch Time = 4.77s: Train Loss = 0.11738, Precision = 69.42%, Recall = 86.61%, F1 = 77.07%, Val Loss = 0.21422, Precision = 52.30%, Recall = 68.28%, F1 = 59.23%\n",
            "Epoch 38 / 100, Epoch Time = 4.72s: Train Loss = 0.11632, Precision = 69.35%, Recall = 87.41%, F1 = 77.34%, Val Loss = 0.21600, Precision = 49.84%, Recall = 69.57%, F1 = 58.08%\n",
            "Epoch 39 / 100, Epoch Time = 4.75s: Train Loss = 0.11349, Precision = 70.16%, Recall = 87.85%, F1 = 78.02%, Val Loss = 0.21800, Precision = 54.82%, Recall = 65.75%, F1 = 59.79%\n",
            "Epoch 40 / 100, Epoch Time = 4.83s: Train Loss = 0.11206, Precision = 70.57%, Recall = 87.54%, F1 = 78.14%, Val Loss = 0.22172, Precision = 57.79%, Recall = 62.31%, F1 = 59.96%\n",
            "Epoch 41 / 100, Epoch Time = 4.71s: Train Loss = 0.11012, Precision = 71.72%, Recall = 88.06%, F1 = 79.06%, Val Loss = 0.21859, Precision = 49.53%, Recall = 70.02%, F1 = 58.02%\n",
            "Epoch 42 / 100, Epoch Time = 4.76s: Train Loss = 0.10955, Precision = 71.40%, Recall = 88.00%, F1 = 78.83%, Val Loss = 0.22056, Precision = 51.44%, Recall = 69.18%, F1 = 59.01%\n",
            "[39 / 133]: Loss = 0.11056, Precision = 87.80%, Recall = 65.45%, F1 = 75.00%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43 / 100, Epoch Time = 4.69s: Train Loss = 0.10725, Precision = 72.07%, Recall = 88.39%, F1 = 79.40%, Val Loss = 0.22391, Precision = 48.27%, Recall = 69.53%, F1 = 56.98%\n",
            "Epoch 44 / 100, Epoch Time = 4.73s: Train Loss = 0.10636, Precision = 72.62%, Recall = 88.46%, F1 = 79.76%, Val Loss = 0.22871, Precision = 58.93%, Recall = 59.75%, F1 = 59.34%\n",
            "Epoch 45 / 100, Epoch Time = 4.73s: Train Loss = 0.10504, Precision = 72.78%, Recall = 88.17%, F1 = 79.74%, Val Loss = 0.22368, Precision = 51.61%, Recall = 68.07%, F1 = 58.71%\n",
            "Epoch 46 / 100, Epoch Time = 4.73s: Train Loss = 0.10390, Precision = 73.12%, Recall = 88.72%, F1 = 80.17%, Val Loss = 0.22494, Precision = 49.61%, Recall = 69.33%, F1 = 57.84%\n",
            "Epoch 47 / 100, Epoch Time = 4.72s: Train Loss = 0.10321, Precision = 73.20%, Recall = 88.23%, F1 = 80.02%, Val Loss = 0.22693, Precision = 57.73%, Recall = 62.19%, F1 = 59.88%\n",
            "Epoch 48 / 100, Epoch Time = 4.76s: Train Loss = 0.10162, Precision = 73.71%, Recall = 88.60%, F1 = 80.47%, Val Loss = 0.22854, Precision = 51.50%, Recall = 67.97%, F1 = 58.60%\n",
            "Epoch 49 / 100, Epoch Time = 4.90s: Train Loss = 0.09973, Precision = 73.90%, Recall = 89.29%, F1 = 80.87%, Val Loss = 0.22908, Precision = 49.47%, Recall = 69.07%, F1 = 57.65%\n",
            "[47 / 133]: Loss = 0.09899, Precision = 95.35%, Recall = 70.69%, F1 = 81.19%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50 / 100, Epoch Time = 4.85s: Train Loss = 0.09824, Precision = 75.04%, Recall = 89.28%, F1 = 81.54%, Val Loss = 0.23224, Precision = 47.21%, Recall = 71.33%, F1 = 56.82%\n",
            "Epoch 51 / 100, Epoch Time = 4.73s: Train Loss = 0.09815, Precision = 74.70%, Recall = 89.06%, F1 = 81.25%, Val Loss = 0.22951, Precision = 51.24%, Recall = 67.28%, F1 = 58.18%\n",
            "Epoch 52 / 100, Epoch Time = 4.72s: Train Loss = 0.09608, Precision = 75.56%, Recall = 89.60%, F1 = 81.98%, Val Loss = 0.23088, Precision = 51.61%, Recall = 67.69%, F1 = 58.57%\n",
            "Epoch 53 / 100, Epoch Time = 4.72s: Train Loss = 0.09489, Precision = 76.03%, Recall = 89.72%, F1 = 82.31%, Val Loss = 0.23379, Precision = 52.13%, Recall = 65.80%, F1 = 58.17%\n",
            "Epoch 54 / 100, Epoch Time = 4.74s: Train Loss = 0.09281, Precision = 76.27%, Recall = 89.64%, F1 = 82.42%, Val Loss = 0.23329, Precision = 49.99%, Recall = 68.37%, F1 = 57.75%\n",
            "Epoch 55 / 100, Epoch Time = 4.77s: Train Loss = 0.09264, Precision = 76.22%, Recall = 89.83%, F1 = 82.46%, Val Loss = 0.23361, Precision = 52.50%, Recall = 65.98%, F1 = 58.48%\n",
            "Epoch 56 / 100, Epoch Time = 4.71s: Train Loss = 0.09259, Precision = 76.58%, Recall = 89.73%, F1 = 82.63%, Val Loss = 0.23733, Precision = 51.53%, Recall = 66.56%, F1 = 58.09%\n",
            "[50 / 133]: Loss = 0.09080, Precision = 93.55%, Recall = 63.04%, F1 = 75.32%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 57 / 100, Epoch Time = 4.76s: Train Loss = 0.09030, Precision = 77.50%, Recall = 90.32%, F1 = 83.42%, Val Loss = 0.24209, Precision = 47.90%, Recall = 68.83%, F1 = 56.49%\n",
            "Epoch 58 / 100, Epoch Time = 4.75s: Train Loss = 0.08890, Precision = 77.62%, Recall = 90.19%, F1 = 83.43%, Val Loss = 0.23813, Precision = 52.24%, Recall = 66.30%, F1 = 58.44%\n",
            "Epoch 59 / 100, Epoch Time = 4.72s: Train Loss = 0.08923, Precision = 76.94%, Recall = 89.89%, F1 = 82.91%, Val Loss = 0.24288, Precision = 56.82%, Recall = 60.63%, F1 = 58.66%\n",
            "Epoch 60 / 100, Epoch Time = 4.73s: Train Loss = 0.08844, Precision = 77.84%, Recall = 90.21%, F1 = 83.57%, Val Loss = 0.23916, Precision = 52.64%, Recall = 65.60%, F1 = 58.41%\n",
            "Epoch 61 / 100, Epoch Time = 4.69s: Train Loss = 0.08674, Precision = 78.12%, Recall = 90.33%, F1 = 83.78%, Val Loss = 0.24087, Precision = 53.22%, Recall = 65.43%, F1 = 58.69%\n",
            "Epoch 62 / 100, Epoch Time = 4.73s: Train Loss = 0.08515, Precision = 78.75%, Recall = 90.65%, F1 = 84.28%, Val Loss = 0.24374, Precision = 53.84%, Recall = 63.39%, F1 = 58.23%\n",
            "Epoch 63 / 100, Epoch Time = 4.81s: Train Loss = 0.08402, Precision = 79.04%, Recall = 90.51%, F1 = 84.39%, Val Loss = 0.24763, Precision = 47.79%, Recall = 69.70%, F1 = 56.70%\n",
            "[48 / 133]: Loss = 0.08390, Precision = 91.49%, Recall = 75.44%, F1 = 82.69%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 64 / 100, Epoch Time = 4.77s: Train Loss = 0.08392, Precision = 78.75%, Recall = 90.66%, F1 = 84.29%, Val Loss = 0.24975, Precision = 48.16%, Recall = 67.62%, F1 = 56.25%\n",
            "Epoch 65 / 100, Epoch Time = 4.74s: Train Loss = 0.08238, Precision = 79.20%, Recall = 91.00%, F1 = 84.69%, Val Loss = 0.24773, Precision = 53.93%, Recall = 63.75%, F1 = 58.43%\n",
            "Epoch 66 / 100, Epoch Time = 4.82s: Train Loss = 0.08130, Precision = 79.91%, Recall = 91.35%, F1 = 85.25%, Val Loss = 0.24926, Precision = 55.04%, Recall = 62.11%, F1 = 58.36%\n",
            "Epoch 67 / 100, Epoch Time = 4.77s: Train Loss = 0.07948, Precision = 80.47%, Recall = 91.38%, F1 = 85.58%, Val Loss = 0.25138, Precision = 50.99%, Recall = 65.54%, F1 = 57.35%\n",
            "Epoch 68 / 100, Epoch Time = 4.74s: Train Loss = 0.08045, Precision = 80.36%, Recall = 91.14%, F1 = 85.41%, Val Loss = 0.25462, Precision = 56.39%, Recall = 59.73%, F1 = 58.01%\n",
            "Epoch 69 / 100, Epoch Time = 4.77s: Train Loss = 0.07997, Precision = 80.40%, Recall = 90.78%, F1 = 85.28%, Val Loss = 0.26020, Precision = 46.81%, Recall = 68.77%, F1 = 55.70%\n",
            "Epoch 70 / 100, Epoch Time = 4.69s: Train Loss = 0.07742, Precision = 81.36%, Recall = 91.83%, F1 = 86.27%, Val Loss = 0.25522, Precision = 54.82%, Recall = 63.24%, F1 = 58.73%\n",
            "[50 / 133]: Loss = 0.07178, Precision = 91.23%, Recall = 83.87%, F1 = 87.39%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 71 / 100, Epoch Time = 4.68s: Train Loss = 0.07639, Precision = 81.24%, Recall = 91.37%, F1 = 86.01%, Val Loss = 0.25898, Precision = 49.59%, Recall = 66.02%, F1 = 56.63%\n",
            "Epoch 72 / 100, Epoch Time = 4.70s: Train Loss = 0.07660, Precision = 81.96%, Recall = 91.64%, F1 = 86.53%, Val Loss = 0.25751, Precision = 51.47%, Recall = 64.64%, F1 = 57.31%\n",
            "Epoch 73 / 100, Epoch Time = 4.77s: Train Loss = 0.07472, Precision = 81.58%, Recall = 91.79%, F1 = 86.38%, Val Loss = 0.26410, Precision = 59.27%, Recall = 57.53%, F1 = 58.39%\n",
            "Epoch 74 / 100, Epoch Time = 4.76s: Train Loss = 0.07401, Precision = 82.21%, Recall = 91.77%, F1 = 86.73%, Val Loss = 0.26076, Precision = 51.73%, Recall = 65.87%, F1 = 57.95%\n",
            "Epoch 75 / 100, Epoch Time = 4.70s: Train Loss = 0.07403, Precision = 81.90%, Recall = 91.95%, F1 = 86.64%, Val Loss = 0.26054, Precision = 52.50%, Recall = 63.52%, F1 = 57.49%\n",
            "Epoch 76 / 100, Epoch Time = 4.66s: Train Loss = 0.07176, Precision = 83.08%, Recall = 92.05%, F1 = 87.34%, Val Loss = 0.26415, Precision = 50.39%, Recall = 65.98%, F1 = 57.14%\n",
            "Epoch 77 / 100, Epoch Time = 4.69s: Train Loss = 0.07289, Precision = 82.50%, Recall = 91.64%, F1 = 86.83%, Val Loss = 0.26848, Precision = 49.01%, Recall = 67.79%, F1 = 56.89%\n",
            "[50 / 133]: Loss = 0.07402, Precision = 92.00%, Recall = 82.14%, F1 = 86.79%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 78 / 100, Epoch Time = 4.69s: Train Loss = 0.07076, Precision = 82.83%, Recall = 92.12%, F1 = 87.23%, Val Loss = 0.26514, Precision = 53.90%, Recall = 61.59%, F1 = 57.49%\n",
            "Epoch 79 / 100, Epoch Time = 4.75s: Train Loss = 0.07019, Precision = 83.16%, Recall = 91.90%, F1 = 87.32%, Val Loss = 0.26649, Precision = 52.27%, Recall = 63.22%, F1 = 57.23%\n",
            "Epoch 80 / 100, Epoch Time = 4.73s: Train Loss = 0.06967, Precision = 83.47%, Recall = 92.43%, F1 = 87.72%, Val Loss = 0.26906, Precision = 53.50%, Recall = 61.30%, F1 = 57.13%\n",
            "Epoch 81 / 100, Epoch Time = 4.70s: Train Loss = 0.06889, Precision = 83.88%, Recall = 92.41%, F1 = 87.94%, Val Loss = 0.27587, Precision = 48.50%, Recall = 66.78%, F1 = 56.19%\n",
            "Epoch 82 / 100, Epoch Time = 4.75s: Train Loss = 0.06841, Precision = 83.63%, Recall = 92.26%, F1 = 87.73%, Val Loss = 0.27299, Precision = 49.67%, Recall = 66.31%, F1 = 56.80%\n",
            "Epoch 83 / 100, Epoch Time = 4.73s: Train Loss = 0.06817, Precision = 84.19%, Recall = 91.98%, F1 = 87.91%, Val Loss = 0.27424, Precision = 51.70%, Recall = 64.06%, F1 = 57.22%\n",
            "Epoch 84 / 100, Epoch Time = 4.78s: Train Loss = 0.06701, Precision = 84.07%, Recall = 92.45%, F1 = 88.06%, Val Loss = 0.27862, Precision = 57.65%, Recall = 56.99%, F1 = 57.32%\n",
            "[43 / 133]: Loss = 0.05836, Precision = 91.07%, Recall = 89.47%, F1 = 90.27%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 85 / 100, Epoch Time = 4.78s: Train Loss = 0.06693, Precision = 84.26%, Recall = 92.50%, F1 = 88.18%, Val Loss = 0.27589, Precision = 55.04%, Recall = 61.67%, F1 = 58.17%\n",
            "Epoch 86 / 100, Epoch Time = 4.82s: Train Loss = 0.06544, Precision = 84.90%, Recall = 92.42%, F1 = 88.50%, Val Loss = 0.28140, Precision = 55.22%, Recall = 59.35%, F1 = 57.21%\n",
            "Epoch 87 / 100, Epoch Time = 4.71s: Train Loss = 0.06389, Precision = 85.35%, Recall = 93.11%, F1 = 89.06%, Val Loss = 0.27817, Precision = 54.19%, Recall = 62.16%, F1 = 57.90%\n",
            "Epoch 88 / 100, Epoch Time = 4.71s: Train Loss = 0.06415, Precision = 85.03%, Recall = 92.98%, F1 = 88.83%, Val Loss = 0.29254, Precision = 59.56%, Recall = 53.99%, F1 = 56.64%\n",
            "Epoch 89 / 100, Epoch Time = 4.80s: Train Loss = 0.06465, Precision = 84.51%, Recall = 92.77%, F1 = 88.45%, Val Loss = 0.28228, Precision = 52.96%, Recall = 62.62%, F1 = 57.39%\n",
            "Epoch 90 / 100, Epoch Time = 4.73s: Train Loss = 0.06283, Precision = 85.26%, Recall = 92.92%, F1 = 88.93%, Val Loss = 0.28701, Precision = 51.39%, Recall = 62.78%, F1 = 56.51%\n",
            "Epoch 91 / 100, Epoch Time = 4.72s: Train Loss = 0.06220, Precision = 85.71%, Recall = 93.12%, F1 = 89.26%, Val Loss = 0.28413, Precision = 55.56%, Recall = 58.98%, F1 = 57.22%\n",
            "[52 / 133]: Loss = 0.08126, Precision = 87.80%, Recall = 76.60%, F1 = 81.82%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 92 / 100, Epoch Time = 4.72s: Train Loss = 0.06099, Precision = 86.20%, Recall = 93.37%, F1 = 89.64%, Val Loss = 0.29575, Precision = 59.85%, Recall = 54.72%, F1 = 57.17%\n",
            "Epoch 93 / 100, Epoch Time = 4.73s: Train Loss = 0.06030, Precision = 86.19%, Recall = 93.14%, F1 = 89.53%, Val Loss = 0.28754, Precision = 54.33%, Recall = 61.30%, F1 = 57.61%\n",
            "Epoch 94 / 100, Epoch Time = 4.76s: Train Loss = 0.06019, Precision = 86.22%, Recall = 93.26%, F1 = 89.60%, Val Loss = 0.28913, Precision = 50.56%, Recall = 65.16%, F1 = 56.94%\n",
            "Epoch 95 / 100, Epoch Time = 4.73s: Train Loss = 0.05935, Precision = 86.15%, Recall = 93.27%, F1 = 89.57%, Val Loss = 0.29080, Precision = 57.10%, Recall = 57.90%, F1 = 57.50%\n",
            "Epoch 96 / 100, Epoch Time = 4.76s: Train Loss = 0.05951, Precision = 86.26%, Recall = 93.18%, F1 = 89.58%, Val Loss = 0.29248, Precision = 53.76%, Recall = 61.47%, F1 = 57.36%\n",
            "Epoch 97 / 100, Epoch Time = 4.79s: Train Loss = 0.05819, Precision = 86.65%, Recall = 93.36%, F1 = 89.88%, Val Loss = 0.29426, Precision = 51.04%, Recall = 61.50%, F1 = 55.79%\n",
            "Epoch 98 / 100, Epoch Time = 4.68s: Train Loss = 0.05790, Precision = 86.69%, Recall = 93.56%, F1 = 90.00%, Val Loss = 0.29679, Precision = 52.27%, Recall = 61.96%, F1 = 56.70%\n",
            "[40 / 133]: Loss = 0.06223, Precision = 100.00%, Recall = 75.71%, F1 = 86.18%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 99 / 100, Epoch Time = 4.74s: Train Loss = 0.05752, Precision = 87.00%, Recall = 93.51%, F1 = 90.14%, Val Loss = 0.29808, Precision = 52.33%, Recall = 61.03%, F1 = 56.35%\n",
            "Epoch 100 / 100, Epoch Time = 4.70s: Train Loss = 0.05664, Precision = 87.16%, Recall = 93.45%, F1 = 90.19%, Val Loss = 0.29714, Precision = 53.33%, Recall = 61.42%, F1 = 57.09%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qstoDysVsSQ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Различают Narrow и Wide свёртки - по сути, добавляется ли нулевой паддинг или нет. Для текстов эта разница выглядит так:  \n",
        "![narrow_vs_wide](https://image.ibb.co/eqGZaS/2018_03_28_11_23_17.png)\n",
        "From Neural Network Methods in Natural Language Processing.  \n",
        "Слева - паддинг отсутствует, справа - есть. Попробуйте добавить паддинг и посмотреть, что получится. Потенциально он поможет выучить хорошие префиксы слова.\n",
        "\n",
        "--- \n",
        "\n",
        "**Задание** Сравните качество и скорость работы с character-level LSTM (типа того, что был на третьем занятии)."
      ]
    },
    {
      "metadata": {
        "id": "Nis9taJf9XFU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PaddingConvClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, filters_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.char_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.cnn = nn.Conv2d(in_channels=1, out_channels=filters_count, kernel_size=(3, emb_dim), padding=(1, 0))\n",
        "        self.pool = nn.MaxPool1d(MAX_WORD_LEN)\n",
        "        self.out = nn.Linear(filters_count, 1)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        '''\n",
        "        inp.size() = (batch_size, max_word_len)\n",
        "        out.size() = (batch_size,)\n",
        "        '''\n",
        "        emb = self.char_emb(inp).unsqueeze(1)\n",
        "        cnn_out = F.relu(self.cnn(emb)).squeeze()\n",
        "        pool_out = self.pool(cnn_out).squeeze()\n",
        "        return self.out(pool_out).squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vtYCpkhG9j8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1843
        },
        "outputId": "59f9ae91-22d3-4406-fa4b-92fcbd187160"
      },
      "cell_type": "code",
      "source": [
        "model = PaddingConvClassifier(len(char_index) + 1, 24, 256).cuda()\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().cuda()\n",
        "\n",
        "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad])\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=100, \n",
        "    batch_size=512, val_data=(X_test, y_test), val_batch_size=1024)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[0 / 133]: Loss = 0.70297, Precision = 7.50%, Recall = 36.21%, F1 = 12.43%\r[1 / 133]: Loss = 0.53678, Precision = 50.00%, Recall = 2.17%, F1 = 4.17%\r[2 / 133]: Loss = 0.43636, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[3 / 133]: Loss = 0.39382, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[4 / 133]: Loss = 0.36311, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[5 / 133]: Loss = 0.33412, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[6 / 133]: Loss = 0.32109, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[7 / 133]: Loss = 0.35937, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 100, Epoch Time = 4.72s: Train Loss = 0.31383, Precision = 0.42%, Recall = 10.17%, F1 = 0.81%, Val Loss = 0.27433, Precision = 1.57%, Recall = 64.71%, F1 = 3.07%\n",
            "Epoch 2 / 100, Epoch Time = 4.72s: Train Loss = 0.25667, Precision = 9.78%, Recall = 66.54%, F1 = 17.05%, Val Loss = 0.24635, Precision = 21.75%, Recall = 63.52%, F1 = 32.40%\n",
            "Epoch 3 / 100, Epoch Time = 4.73s: Train Loss = 0.23369, Precision = 24.04%, Recall = 69.03%, F1 = 35.66%, Val Loss = 0.23292, Precision = 27.15%, Recall = 65.97%, F1 = 38.47%\n",
            "Epoch 4 / 100, Epoch Time = 4.67s: Train Loss = 0.22085, Precision = 30.91%, Recall = 70.81%, F1 = 43.04%, Val Loss = 0.22606, Precision = 29.64%, Recall = 68.22%, F1 = 41.32%\n",
            "Epoch 5 / 100, Epoch Time = 4.74s: Train Loss = 0.21228, Precision = 35.61%, Recall = 72.05%, F1 = 47.66%, Val Loss = 0.22151, Precision = 34.75%, Recall = 68.51%, F1 = 46.11%\n",
            "Epoch 6 / 100, Epoch Time = 4.67s: Train Loss = 0.20592, Precision = 38.69%, Recall = 73.46%, F1 = 50.68%, Val Loss = 0.21918, Precision = 32.01%, Recall = 71.98%, F1 = 44.31%\n",
            "Epoch 7 / 100, Epoch Time = 4.72s: Train Loss = 0.20050, Precision = 41.20%, Recall = 74.64%, F1 = 53.09%, Val Loss = 0.21534, Precision = 41.61%, Recall = 66.88%, F1 = 51.30%\n",
            "[55 / 133]: Loss = 0.16116, Precision = 85.19%, Recall = 44.23%, F1 = 58.23%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8 / 100, Epoch Time = 4.64s: Train Loss = 0.19588, Precision = 42.99%, Recall = 76.17%, F1 = 54.96%, Val Loss = 0.21664, Precision = 48.56%, Recall = 64.48%, F1 = 55.40%\n",
            "Epoch 9 / 100, Epoch Time = 4.68s: Train Loss = 0.19275, Precision = 44.78%, Recall = 75.94%, F1 = 56.34%, Val Loss = 0.21120, Precision = 41.21%, Recall = 69.80%, F1 = 51.82%\n",
            "Epoch 10 / 100, Epoch Time = 4.66s: Train Loss = 0.18756, Precision = 46.76%, Recall = 77.02%, F1 = 58.19%, Val Loss = 0.21124, Precision = 44.30%, Recall = 68.34%, F1 = 53.75%\n",
            "Epoch 11 / 100, Epoch Time = 4.67s: Train Loss = 0.18408, Precision = 48.45%, Recall = 77.59%, F1 = 59.65%, Val Loss = 0.20882, Precision = 42.76%, Recall = 69.81%, F1 = 53.03%\n",
            "Epoch 12 / 100, Epoch Time = 4.73s: Train Loss = 0.18151, Precision = 49.04%, Recall = 78.04%, F1 = 60.23%, Val Loss = 0.20830, Precision = 44.93%, Recall = 69.87%, F1 = 54.69%\n",
            "Epoch 13 / 100, Epoch Time = 4.73s: Train Loss = 0.17720, Precision = 50.88%, Recall = 78.89%, F1 = 61.86%, Val Loss = 0.20764, Precision = 41.50%, Recall = 71.49%, F1 = 52.51%\n",
            "Epoch 14 / 100, Epoch Time = 4.75s: Train Loss = 0.17444, Precision = 51.87%, Recall = 79.55%, F1 = 62.79%, Val Loss = 0.20852, Precision = 42.41%, Recall = 71.93%, F1 = 53.36%\n",
            "[43 / 133]: Loss = 0.18134, Precision = 72.55%, Recall = 63.79%, F1 = 67.89%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15 / 100, Epoch Time = 4.75s: Train Loss = 0.17012, Precision = 53.17%, Recall = 80.07%, F1 = 63.91%, Val Loss = 0.20653, Precision = 45.24%, Recall = 70.11%, F1 = 54.99%\n",
            "Epoch 16 / 100, Epoch Time = 4.74s: Train Loss = 0.16750, Precision = 54.41%, Recall = 80.53%, F1 = 64.94%, Val Loss = 0.20706, Precision = 48.53%, Recall = 69.62%, F1 = 57.19%\n",
            "Epoch 17 / 100, Epoch Time = 4.70s: Train Loss = 0.16546, Precision = 55.19%, Recall = 81.20%, F1 = 65.72%, Val Loss = 0.20719, Precision = 45.67%, Recall = 69.93%, F1 = 55.26%\n",
            "Epoch 18 / 100, Epoch Time = 4.77s: Train Loss = 0.16164, Precision = 55.75%, Recall = 81.84%, F1 = 66.32%, Val Loss = 0.21083, Precision = 54.70%, Recall = 64.44%, F1 = 59.17%\n",
            "Epoch 19 / 100, Epoch Time = 4.73s: Train Loss = 0.15930, Precision = 56.87%, Recall = 82.19%, F1 = 67.22%, Val Loss = 0.20652, Precision = 44.58%, Recall = 70.46%, F1 = 54.61%\n",
            "Epoch 20 / 100, Epoch Time = 4.68s: Train Loss = 0.15597, Precision = 57.80%, Recall = 82.31%, F1 = 67.91%, Val Loss = 0.20651, Precision = 47.24%, Recall = 69.98%, F1 = 56.41%\n",
            "Epoch 21 / 100, Epoch Time = 4.77s: Train Loss = 0.15437, Precision = 58.31%, Recall = 82.63%, F1 = 68.37%, Val Loss = 0.20666, Precision = 44.01%, Recall = 72.99%, F1 = 54.91%\n",
            "[54 / 133]: Loss = 0.16130, Precision = 81.40%, Recall = 66.04%, F1 = 72.92%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22 / 100, Epoch Time = 4.74s: Train Loss = 0.15085, Precision = 58.93%, Recall = 82.90%, F1 = 68.89%, Val Loss = 0.21062, Precision = 54.79%, Recall = 64.87%, F1 = 59.41%\n",
            "Epoch 23 / 100, Epoch Time = 4.74s: Train Loss = 0.14980, Precision = 59.70%, Recall = 82.84%, F1 = 69.39%, Val Loss = 0.20712, Precision = 51.24%, Recall = 68.64%, F1 = 58.68%\n",
            "Epoch 24 / 100, Epoch Time = 4.80s: Train Loss = 0.14571, Precision = 60.92%, Recall = 84.14%, F1 = 70.67%, Val Loss = 0.21165, Precision = 40.84%, Recall = 75.09%, F1 = 52.91%\n",
            "Epoch 25 / 100, Epoch Time = 4.71s: Train Loss = 0.14441, Precision = 61.42%, Recall = 84.00%, F1 = 70.96%, Val Loss = 0.21035, Precision = 53.56%, Recall = 65.48%, F1 = 58.92%\n",
            "Epoch 26 / 100, Epoch Time = 4.70s: Train Loss = 0.14136, Precision = 61.86%, Recall = 84.34%, F1 = 71.37%, Val Loss = 0.21509, Precision = 57.87%, Recall = 62.69%, F1 = 60.19%\n",
            "Epoch 27 / 100, Epoch Time = 4.70s: Train Loss = 0.14046, Precision = 62.54%, Recall = 83.60%, F1 = 71.55%, Val Loss = 0.21501, Precision = 41.84%, Recall = 75.70%, F1 = 53.89%\n",
            "Epoch 28 / 100, Epoch Time = 4.71s: Train Loss = 0.13751, Precision = 63.57%, Recall = 84.79%, F1 = 72.66%, Val Loss = 0.20971, Precision = 52.07%, Recall = 67.28%, F1 = 58.71%\n",
            "[53 / 133]: Loss = 0.14729, Precision = 88.89%, Recall = 64.52%, F1 = 74.77%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29 / 100, Epoch Time = 4.72s: Train Loss = 0.13547, Precision = 64.45%, Recall = 85.57%, F1 = 73.52%, Val Loss = 0.20974, Precision = 49.59%, Recall = 68.66%, F1 = 57.58%\n",
            "Epoch 30 / 100, Epoch Time = 4.82s: Train Loss = 0.13301, Precision = 64.71%, Recall = 85.70%, F1 = 73.74%, Val Loss = 0.21183, Precision = 53.39%, Recall = 66.34%, F1 = 59.16%\n",
            "Epoch 31 / 100, Epoch Time = 4.68s: Train Loss = 0.13139, Precision = 65.25%, Recall = 85.77%, F1 = 74.11%, Val Loss = 0.21093, Precision = 52.56%, Recall = 67.46%, F1 = 59.08%\n",
            "Epoch 32 / 100, Epoch Time = 4.69s: Train Loss = 0.12939, Precision = 66.07%, Recall = 86.08%, F1 = 74.76%, Val Loss = 0.21642, Precision = 44.64%, Recall = 72.72%, F1 = 55.32%\n",
            "Epoch 33 / 100, Epoch Time = 4.72s: Train Loss = 0.12827, Precision = 66.16%, Recall = 86.22%, F1 = 74.87%, Val Loss = 0.21531, Precision = 43.10%, Recall = 73.70%, F1 = 54.39%\n",
            "Epoch 34 / 100, Epoch Time = 4.76s: Train Loss = 0.12595, Precision = 67.01%, Recall = 86.34%, F1 = 75.46%, Val Loss = 0.21544, Precision = 55.79%, Recall = 64.57%, F1 = 59.86%\n",
            "Epoch 35 / 100, Epoch Time = 4.71s: Train Loss = 0.12363, Precision = 67.94%, Recall = 86.84%, F1 = 76.23%, Val Loss = 0.21459, Precision = 49.96%, Recall = 68.12%, F1 = 57.64%\n",
            "[54 / 133]: Loss = 0.15409, Precision = 90.00%, Recall = 57.14%, F1 = 69.90%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36 / 100, Epoch Time = 4.69s: Train Loss = 0.12096, Precision = 68.19%, Recall = 87.43%, F1 = 76.62%, Val Loss = 0.21660, Precision = 46.13%, Recall = 71.23%, F1 = 55.99%\n",
            "Epoch 37 / 100, Epoch Time = 4.74s: Train Loss = 0.12060, Precision = 68.50%, Recall = 87.51%, F1 = 76.84%, Val Loss = 0.21892, Precision = 45.58%, Recall = 72.60%, F1 = 56.00%\n",
            "Epoch 38 / 100, Epoch Time = 4.69s: Train Loss = 0.11800, Precision = 69.55%, Recall = 87.66%, F1 = 77.56%, Val Loss = 0.21825, Precision = 57.22%, Recall = 63.45%, F1 = 60.17%\n",
            "Epoch 39 / 100, Epoch Time = 4.72s: Train Loss = 0.11594, Precision = 69.90%, Recall = 87.96%, F1 = 77.90%, Val Loss = 0.21906, Precision = 47.33%, Recall = 69.93%, F1 = 56.45%\n",
            "Epoch 40 / 100, Epoch Time = 4.71s: Train Loss = 0.11547, Precision = 69.86%, Recall = 87.99%, F1 = 77.88%, Val Loss = 0.21779, Precision = 50.39%, Recall = 68.55%, F1 = 58.08%\n",
            "Epoch 41 / 100, Epoch Time = 4.75s: Train Loss = 0.11382, Precision = 71.12%, Recall = 87.97%, F1 = 78.65%, Val Loss = 0.22423, Precision = 42.87%, Recall = 71.94%, F1 = 53.72%\n",
            "Epoch 42 / 100, Epoch Time = 4.77s: Train Loss = 0.11281, Precision = 70.43%, Recall = 87.99%, F1 = 78.24%, Val Loss = 0.22335, Precision = 58.05%, Recall = 61.43%, F1 = 59.69%\n",
            "[48 / 133]: Loss = 0.08654, Precision = 95.24%, Recall = 78.43%, F1 = 86.02%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43 / 100, Epoch Time = 4.74s: Train Loss = 0.11173, Precision = 71.73%, Recall = 88.07%, F1 = 79.06%, Val Loss = 0.22772, Precision = 58.99%, Recall = 60.83%, F1 = 59.90%\n",
            "Epoch 44 / 100, Epoch Time = 4.74s: Train Loss = 0.11021, Precision = 71.89%, Recall = 88.06%, F1 = 79.15%, Val Loss = 0.22177, Precision = 53.99%, Recall = 64.74%, F1 = 58.87%\n",
            "Epoch 45 / 100, Epoch Time = 4.68s: Train Loss = 0.10662, Precision = 72.91%, Recall = 89.33%, F1 = 80.29%, Val Loss = 0.22359, Precision = 50.81%, Recall = 66.47%, F1 = 57.60%\n",
            "Epoch 46 / 100, Epoch Time = 4.68s: Train Loss = 0.10685, Precision = 73.08%, Recall = 88.41%, F1 = 80.02%, Val Loss = 0.22675, Precision = 55.93%, Recall = 62.27%, F1 = 58.93%\n",
            "Epoch 47 / 100, Epoch Time = 4.75s: Train Loss = 0.10443, Precision = 73.78%, Recall = 89.06%, F1 = 80.70%, Val Loss = 0.22478, Precision = 52.84%, Recall = 64.45%, F1 = 58.07%\n",
            "Epoch 48 / 100, Epoch Time = 4.78s: Train Loss = 0.10443, Precision = 73.69%, Recall = 89.05%, F1 = 80.65%, Val Loss = 0.22608, Precision = 52.87%, Recall = 64.89%, F1 = 58.27%\n",
            "Epoch 49 / 100, Epoch Time = 4.71s: Train Loss = 0.10268, Precision = 74.00%, Recall = 89.54%, F1 = 81.03%, Val Loss = 0.22903, Precision = 48.99%, Recall = 67.83%, F1 = 56.89%\n",
            "[50 / 133]: Loss = 0.12206, Precision = 88.46%, Recall = 77.97%, F1 = 82.88%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50 / 100, Epoch Time = 4.72s: Train Loss = 0.10068, Precision = 74.86%, Recall = 89.53%, F1 = 81.54%, Val Loss = 0.22755, Precision = 52.50%, Recall = 65.56%, F1 = 58.31%\n",
            "Epoch 51 / 100, Epoch Time = 4.72s: Train Loss = 0.09922, Precision = 75.19%, Recall = 89.35%, F1 = 81.66%, Val Loss = 0.22951, Precision = 51.39%, Recall = 64.77%, F1 = 57.31%\n",
            "Epoch 52 / 100, Epoch Time = 4.73s: Train Loss = 0.09826, Precision = 75.42%, Recall = 90.03%, F1 = 82.08%, Val Loss = 0.23252, Precision = 46.76%, Recall = 69.94%, F1 = 56.05%\n",
            "Epoch 53 / 100, Epoch Time = 4.75s: Train Loss = 0.09675, Precision = 75.50%, Recall = 89.64%, F1 = 81.96%, Val Loss = 0.23043, Precision = 51.10%, Recall = 65.95%, F1 = 57.58%\n",
            "Epoch 54 / 100, Epoch Time = 4.73s: Train Loss = 0.09720, Precision = 75.78%, Recall = 89.41%, F1 = 82.03%, Val Loss = 0.24303, Precision = 43.96%, Recall = 72.21%, F1 = 54.65%\n",
            "Epoch 55 / 100, Epoch Time = 4.72s: Train Loss = 0.09553, Precision = 76.41%, Recall = 89.79%, F1 = 82.56%, Val Loss = 0.23438, Precision = 50.27%, Recall = 66.35%, F1 = 57.20%\n",
            "Epoch 56 / 100, Epoch Time = 4.77s: Train Loss = 0.09332, Precision = 76.55%, Recall = 90.39%, F1 = 82.90%, Val Loss = 0.23529, Precision = 53.76%, Recall = 63.10%, F1 = 58.06%\n",
            "[43 / 133]: Loss = 0.09959, Precision = 96.67%, Recall = 67.44%, F1 = 79.45%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 57 / 100, Epoch Time = 4.77s: Train Loss = 0.09306, Precision = 77.01%, Recall = 89.92%, F1 = 82.97%, Val Loss = 0.23795, Precision = 54.07%, Recall = 62.48%, F1 = 57.97%\n",
            "Epoch 58 / 100, Epoch Time = 4.71s: Train Loss = 0.09084, Precision = 77.52%, Recall = 90.57%, F1 = 83.54%, Val Loss = 0.23673, Precision = 55.64%, Recall = 62.03%, F1 = 58.66%\n",
            "Epoch 59 / 100, Epoch Time = 4.71s: Train Loss = 0.09110, Precision = 77.81%, Recall = 90.47%, F1 = 83.67%, Val Loss = 0.24030, Precision = 48.41%, Recall = 68.58%, F1 = 56.76%\n",
            "Epoch 60 / 100, Epoch Time = 4.70s: Train Loss = 0.08861, Precision = 78.04%, Recall = 91.07%, F1 = 84.05%, Val Loss = 0.23987, Precision = 51.27%, Recall = 64.32%, F1 = 57.06%\n",
            "Epoch 61 / 100, Epoch Time = 4.75s: Train Loss = 0.08802, Precision = 78.15%, Recall = 90.82%, F1 = 84.01%, Val Loss = 0.24377, Precision = 57.33%, Recall = 59.56%, F1 = 58.42%\n",
            "Epoch 62 / 100, Epoch Time = 4.73s: Train Loss = 0.08616, Precision = 78.92%, Recall = 90.93%, F1 = 84.50%, Val Loss = 0.24092, Precision = 52.90%, Recall = 63.30%, F1 = 57.64%\n",
            "Epoch 63 / 100, Epoch Time = 4.68s: Train Loss = 0.08845, Precision = 77.90%, Recall = 90.41%, F1 = 83.69%, Val Loss = 0.24525, Precision = 48.93%, Recall = 68.40%, F1 = 57.05%\n",
            "[45 / 133]: Loss = 0.07299, Precision = 84.91%, Recall = 88.24%, F1 = 86.54%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 64 / 100, Epoch Time = 4.69s: Train Loss = 0.08890, Precision = 78.30%, Recall = 89.74%, F1 = 83.63%, Val Loss = 0.24405, Precision = 51.87%, Recall = 64.36%, F1 = 57.45%\n",
            "Epoch 65 / 100, Epoch Time = 4.72s: Train Loss = 0.08501, Precision = 79.20%, Recall = 90.84%, F1 = 84.62%, Val Loss = 0.24886, Precision = 58.30%, Recall = 59.46%, F1 = 58.87%\n",
            "Epoch 66 / 100, Epoch Time = 4.71s: Train Loss = 0.08295, Precision = 80.11%, Recall = 91.28%, F1 = 85.33%, Val Loss = 0.24584, Precision = 52.87%, Recall = 62.78%, F1 = 57.40%\n",
            "Epoch 67 / 100, Epoch Time = 4.71s: Train Loss = 0.08221, Precision = 80.10%, Recall = 91.28%, F1 = 85.32%, Val Loss = 0.24775, Precision = 53.36%, Recall = 63.37%, F1 = 57.94%\n",
            "Epoch 68 / 100, Epoch Time = 4.79s: Train Loss = 0.08184, Precision = 80.35%, Recall = 91.47%, F1 = 85.55%, Val Loss = 0.24938, Precision = 51.10%, Recall = 64.43%, F1 = 57.00%\n",
            "Epoch 69 / 100, Epoch Time = 4.77s: Train Loss = 0.08062, Precision = 81.12%, Recall = 91.38%, F1 = 85.95%, Val Loss = 0.25460, Precision = 56.84%, Recall = 59.13%, F1 = 57.96%\n",
            "Epoch 70 / 100, Epoch Time = 4.81s: Train Loss = 0.07930, Precision = 80.99%, Recall = 92.10%, F1 = 86.19%, Val Loss = 0.25136, Precision = 53.90%, Recall = 62.22%, F1 = 57.76%\n",
            "[37 / 133]: Loss = 0.07032, Precision = 96.30%, Recall = 86.67%, F1 = 91.23%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 71 / 100, Epoch Time = 4.86s: Train Loss = 0.07808, Precision = 81.47%, Recall = 91.65%, F1 = 86.26%, Val Loss = 0.25120, Precision = 53.53%, Recall = 62.71%, F1 = 57.76%\n",
            "Epoch 72 / 100, Epoch Time = 4.83s: Train Loss = 0.07877, Precision = 81.85%, Recall = 91.35%, F1 = 86.34%, Val Loss = 0.25439, Precision = 53.62%, Recall = 62.39%, F1 = 57.67%\n",
            "Epoch 73 / 100, Epoch Time = 4.74s: Train Loss = 0.07664, Precision = 82.04%, Recall = 92.05%, F1 = 86.76%, Val Loss = 0.25533, Precision = 52.70%, Recall = 62.72%, F1 = 57.28%\n",
            "Epoch 74 / 100, Epoch Time = 4.67s: Train Loss = 0.07566, Precision = 82.11%, Recall = 92.16%, F1 = 86.84%, Val Loss = 0.25651, Precision = 53.96%, Recall = 61.26%, F1 = 57.38%\n",
            "Epoch 75 / 100, Epoch Time = 4.68s: Train Loss = 0.07559, Precision = 82.18%, Recall = 91.79%, F1 = 86.72%, Val Loss = 0.25726, Precision = 52.39%, Recall = 63.65%, F1 = 57.47%\n",
            "Epoch 76 / 100, Epoch Time = 4.76s: Train Loss = 0.07356, Precision = 82.88%, Recall = 92.31%, F1 = 87.34%, Val Loss = 0.26414, Precision = 47.47%, Recall = 67.58%, F1 = 55.77%\n",
            "Epoch 77 / 100, Epoch Time = 4.68s: Train Loss = 0.07299, Precision = 82.81%, Recall = 92.51%, F1 = 87.39%, Val Loss = 0.26315, Precision = 58.02%, Recall = 58.38%, F1 = 58.20%\n",
            "[50 / 133]: Loss = 0.08358, Precision = 94.12%, Recall = 78.69%, F1 = 85.71%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 78 / 100, Epoch Time = 4.67s: Train Loss = 0.07276, Precision = 83.11%, Recall = 92.03%, F1 = 87.34%, Val Loss = 0.26239, Precision = 51.90%, Recall = 62.95%, F1 = 56.89%\n",
            "Epoch 79 / 100, Epoch Time = 4.72s: Train Loss = 0.07160, Precision = 83.39%, Recall = 92.62%, F1 = 87.76%, Val Loss = 0.26694, Precision = 52.62%, Recall = 62.32%, F1 = 57.06%\n",
            "Epoch 80 / 100, Epoch Time = 4.68s: Train Loss = 0.07087, Precision = 83.06%, Recall = 92.58%, F1 = 87.57%, Val Loss = 0.26895, Precision = 59.30%, Recall = 57.85%, F1 = 58.57%\n",
            "Epoch 81 / 100, Epoch Time = 4.68s: Train Loss = 0.07054, Precision = 83.89%, Recall = 92.37%, F1 = 87.92%, Val Loss = 0.26833, Precision = 50.36%, Recall = 64.49%, F1 = 56.56%\n",
            "Epoch 82 / 100, Epoch Time = 4.67s: Train Loss = 0.06941, Precision = 83.78%, Recall = 92.56%, F1 = 87.95%, Val Loss = 0.27766, Precision = 61.13%, Recall = 54.26%, F1 = 57.49%\n",
            "Epoch 83 / 100, Epoch Time = 4.63s: Train Loss = 0.06962, Precision = 84.14%, Recall = 92.06%, F1 = 87.92%, Val Loss = 0.27109, Precision = 48.99%, Recall = 64.90%, F1 = 55.83%\n",
            "Epoch 84 / 100, Epoch Time = 4.66s: Train Loss = 0.06837, Precision = 84.47%, Recall = 92.54%, F1 = 88.32%, Val Loss = 0.27410, Precision = 49.93%, Recall = 63.27%, F1 = 55.81%\n",
            "[54 / 133]: Loss = 0.06908, Precision = 98.15%, Recall = 89.83%, F1 = 93.81%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 85 / 100, Epoch Time = 4.77s: Train Loss = 0.06629, Precision = 84.75%, Recall = 93.05%, F1 = 88.70%, Val Loss = 0.27282, Precision = 49.56%, Recall = 65.43%, F1 = 56.40%\n",
            "Epoch 86 / 100, Epoch Time = 4.76s: Train Loss = 0.06652, Precision = 84.86%, Recall = 92.94%, F1 = 88.72%, Val Loss = 0.27932, Precision = 58.67%, Recall = 56.90%, F1 = 57.77%\n",
            "Epoch 87 / 100, Epoch Time = 4.72s: Train Loss = 0.06687, Precision = 84.86%, Recall = 92.35%, F1 = 88.44%, Val Loss = 0.28244, Precision = 48.04%, Recall = 64.93%, F1 = 55.22%\n",
            "Epoch 88 / 100, Epoch Time = 4.76s: Train Loss = 0.06558, Precision = 84.87%, Recall = 92.79%, F1 = 88.65%, Val Loss = 0.27832, Precision = 51.24%, Recall = 62.63%, F1 = 56.37%\n",
            "Epoch 89 / 100, Epoch Time = 4.71s: Train Loss = 0.06458, Precision = 85.47%, Recall = 93.03%, F1 = 89.09%, Val Loss = 0.28209, Precision = 50.61%, Recall = 64.19%, F1 = 56.60%\n",
            "Epoch 90 / 100, Epoch Time = 4.75s: Train Loss = 0.06441, Precision = 84.76%, Recall = 93.16%, F1 = 88.76%, Val Loss = 0.28121, Precision = 56.87%, Recall = 58.34%, F1 = 57.60%\n",
            "Epoch 91 / 100, Epoch Time = 4.72s: Train Loss = 0.06325, Precision = 86.01%, Recall = 92.71%, F1 = 89.23%, Val Loss = 0.28411, Precision = 58.30%, Recall = 58.07%, F1 = 58.19%\n",
            "[51 / 133]: Loss = 0.05186, Precision = 95.83%, Recall = 88.46%, F1 = 92.00%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 92 / 100, Epoch Time = 4.72s: Train Loss = 0.06225, Precision = 86.19%, Recall = 93.41%, F1 = 89.65%, Val Loss = 0.28300, Precision = 54.93%, Recall = 58.90%, F1 = 56.85%\n",
            "Epoch 93 / 100, Epoch Time = 4.73s: Train Loss = 0.06289, Precision = 85.61%, Recall = 93.40%, F1 = 89.34%, Val Loss = 0.28489, Precision = 54.22%, Recall = 60.45%, F1 = 57.16%\n",
            "Epoch 94 / 100, Epoch Time = 4.69s: Train Loss = 0.06045, Precision = 86.43%, Recall = 93.84%, F1 = 89.98%, Val Loss = 0.28730, Precision = 52.64%, Recall = 61.71%, F1 = 56.82%\n",
            "Epoch 95 / 100, Epoch Time = 4.77s: Train Loss = 0.06050, Precision = 86.48%, Recall = 93.72%, F1 = 89.95%, Val Loss = 0.28835, Precision = 51.36%, Recall = 62.79%, F1 = 56.50%\n",
            "Epoch 96 / 100, Epoch Time = 4.70s: Train Loss = 0.05877, Precision = 87.20%, Recall = 94.05%, F1 = 90.49%, Val Loss = 0.28940, Precision = 50.61%, Recall = 62.54%, F1 = 55.95%\n",
            "Epoch 97 / 100, Epoch Time = 4.72s: Train Loss = 0.05958, Precision = 86.47%, Recall = 93.33%, F1 = 89.77%, Val Loss = 0.30805, Precision = 43.87%, Recall = 69.02%, F1 = 53.64%\n",
            "Epoch 98 / 100, Epoch Time = 4.76s: Train Loss = 0.06001, Precision = 86.72%, Recall = 93.43%, F1 = 89.95%, Val Loss = 0.29468, Precision = 54.90%, Recall = 58.55%, F1 = 56.67%\n",
            "[47 / 133]: Loss = 0.04142, Precision = 97.78%, Recall = 89.80%, F1 = 93.62%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 99 / 100, Epoch Time = 4.73s: Train Loss = 0.05785, Precision = 87.45%, Recall = 94.06%, F1 = 90.64%, Val Loss = 0.29422, Precision = 58.65%, Recall = 57.06%, F1 = 57.84%\n",
            "Epoch 100 / 100, Epoch Time = 4.72s: Train Loss = 0.05772, Precision = 87.16%, Recall = 93.97%, F1 = 90.44%, Val Loss = 0.30127, Precision = 61.02%, Recall = 55.79%, F1 = 58.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uk39RsnJIJNE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ]
    },
    {
      "metadata": {
        "id": "Ropr-4HkG-Dv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
        "    epoch_loss = 0\n",
        "    epoch_tp = 0\n",
        "    epoch_tpfn = 0\n",
        "    epoch_tpfp = 0\n",
        "    \n",
        "    model.train(not optimizer is None)\n",
        "    \n",
        "    batchs_count = math.ceil(data[0].shape[0] / batch_size)\n",
        "    \n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "        model.hidden = model.init_hidden(y_batch.size()[0])\n",
        "        \n",
        "        logits = model(X_batch.transpose(1, 0))\n",
        "        \n",
        "        loss = criterion(logits, y_batch)\n",
        "        epoch_loss += loss.data[0]\n",
        "        \n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        preds = (logits.sigmoid() > 0.5).long()\n",
        "        epoch_tp += (preds * y_batch.long()).sum()\n",
        "        epoch_tpfn += (y_batch.long() * (1 - preds)).sum()\n",
        "        epoch_tpfp += ((1 - y_batch.long()) * preds).sum()\n",
        "    \n",
        "        \n",
        "        precision = precision_score(y_batch.data, preds.data)\n",
        "        recall = recall_score(y_batch.data, preds.data)\n",
        "        f1 = f1_score(y_batch.data, preds.data)\n",
        "      \n",
        "        print('\\r[{} / {}]: Loss = {:.5f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'.format(\n",
        "              i, batchs_count, loss.data[0], precision, recall, f1), end='')\n",
        "        \n",
        "    \n",
        "    epoch_tp = epoch_tp.data[0]\n",
        "    epoch_tpfn = epoch_tpfn.data[0]\n",
        "    epoch_tpfp = epoch_tpfp.data[0]\n",
        "    if epoch_tp == 0:\n",
        "      precision = recall = f1 = 0\n",
        "    else:\n",
        "      precision = epoch_tp / (epoch_tp + epoch_tpfp)\n",
        "      recall = epoch_tp / (epoch_tp + epoch_tpfn)\n",
        "      f1 = 2 * precision *  recall / (precision + recall)\n",
        "    \n",
        "    return epoch_loss / batchs_count, recall, precision, f1\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
        "        batch_size=32, val_data=None, val_batch_size=None):\n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_recall, train_precision, train_f1 = \\\n",
        "            do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
        "        \n",
        "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.5f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
        "        if not val_data is None:\n",
        "            val_loss, val_recall, val_precision, val_f1 = \\\n",
        "                do_epoch(model, criterion, val_data, val_batch_size, None)\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            output_info += ', Val Loss = {:.5f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
        "                                     train_loss, train_recall, train_precision, train_f1,\n",
        "                                     val_loss, val_recall, val_precision, val_f1))\n",
        "        else:\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R0lUdf1YFTyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Bidirectional\n",
        "class BidirectionalLstmClassifier(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_embedding_dim, \n",
        "                 lstm_hidden_dim, classes_count, batch_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
        "        self.lstm = nn.LSTM(char_embedding_dim, lstm_hidden_dim, bidirectional=True)\n",
        "        self.output = nn.Linear(2*lstm_hidden_dim, classes_count)\n",
        "        \n",
        "        self.hidden = self.init_hidden(batch_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, word):\n",
        "        chars = self.char_embedding(word)\n",
        "        \n",
        "        _, self.hidden = self.lstm(chars, self.hidden)\n",
        "        fwd = self.hidden[0][0]\n",
        "        bck = self.hidden[0][1]\n",
        "        pred = self.output(torch.cat((fwd, bck), dim=1))\n",
        "        return pred.squeeze()\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        return (Variable(torch.zeros(2, batch_size, self.lstm_hidden_dim).cuda()),\n",
        "                Variable(torch.zeros(2, batch_size, self.lstm_hidden_dim).cuda()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sDaxjXQCFe40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1843
        },
        "outputId": "ad4ab896-842a-4a37-9a3c-0ec748b4102a"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "model = BidirectionalLstmClassifier(len(char_index) + 1, 32, 64, 1, BATCH_SIZE)\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().cuda()\n",
        "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad])\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=100, \n",
        "    batch_size=BATCH_SIZE, val_data=(X_test, y_test), val_batch_size=BATCH_SIZE)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[0 / 133]: Loss = 0.67271, Precision = 2.04%, Recall = 1.92%, F1 = 1.98%\r[1 / 133]: Loss = 0.64814, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[2 / 133]: Loss = 0.61816, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[3 / 133]: Loss = 0.58332, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[4 / 133]: Loss = 0.57078, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[5 / 133]: Loss = 0.55015, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%\r[6 / 133]: Loss = 0.53680, Precision = 0.00%, Recall = 0.00%, F1 = 0.00%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 100, Epoch Time = 4.55s: Train Loss = 0.32885, Precision = 0.36%, Recall = 25.49%, F1 = 0.72%, Val Loss = 0.26354, Precision = 3.80%, Recall = 62.74%, F1 = 7.17%\n",
            "Epoch 2 / 100, Epoch Time = 4.40s: Train Loss = 0.24679, Precision = 17.93%, Recall = 62.96%, F1 = 27.91%, Val Loss = 0.23949, Precision = 21.66%, Recall = 63.97%, F1 = 32.37%\n",
            "Epoch 3 / 100, Epoch Time = 4.36s: Train Loss = 0.23094, Precision = 31.22%, Recall = 65.69%, F1 = 42.33%, Val Loss = 0.22786, Precision = 33.47%, Recall = 65.13%, F1 = 44.21%\n",
            "Epoch 4 / 100, Epoch Time = 4.46s: Train Loss = 0.22278, Precision = 35.29%, Recall = 68.40%, F1 = 46.55%, Val Loss = 0.22244, Precision = 38.98%, Recall = 67.32%, F1 = 49.38%\n",
            "Epoch 5 / 100, Epoch Time = 4.39s: Train Loss = 0.21667, Precision = 38.44%, Recall = 69.96%, F1 = 49.62%, Val Loss = 0.22286, Precision = 47.16%, Recall = 62.05%, F1 = 53.59%\n",
            "Epoch 6 / 100, Epoch Time = 4.46s: Train Loss = 0.21265, Precision = 40.08%, Recall = 71.60%, F1 = 51.39%, Val Loss = 0.21626, Precision = 38.78%, Recall = 68.74%, F1 = 49.59%\n",
            "Epoch 7 / 100, Epoch Time = 4.54s: Train Loss = 0.20740, Precision = 42.11%, Recall = 72.49%, F1 = 53.27%, Val Loss = 0.21544, Precision = 32.09%, Recall = 75.47%, F1 = 45.04%\n",
            "[67 / 133]: Loss = 0.16848, Precision = 66.67%, Recall = 54.17%, F1 = 59.77%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8 / 100, Epoch Time = 4.38s: Train Loss = 0.20432, Precision = 42.84%, Recall = 73.18%, F1 = 54.04%, Val Loss = 0.21224, Precision = 37.67%, Recall = 72.22%, F1 = 49.51%\n",
            "Epoch 9 / 100, Epoch Time = 4.43s: Train Loss = 0.20159, Precision = 43.65%, Recall = 73.79%, F1 = 54.85%, Val Loss = 0.21027, Precision = 48.96%, Recall = 66.04%, F1 = 56.23%\n",
            "Epoch 10 / 100, Epoch Time = 4.40s: Train Loss = 0.19910, Precision = 45.15%, Recall = 73.65%, F1 = 55.98%, Val Loss = 0.20769, Precision = 45.84%, Recall = 68.87%, F1 = 55.04%\n",
            "Epoch 11 / 100, Epoch Time = 4.59s: Train Loss = 0.19498, Precision = 46.44%, Recall = 74.95%, F1 = 57.34%, Val Loss = 0.20715, Precision = 48.47%, Recall = 66.69%, F1 = 56.14%\n",
            "Epoch 12 / 100, Epoch Time = 4.49s: Train Loss = 0.19214, Precision = 47.00%, Recall = 75.72%, F1 = 57.99%, Val Loss = 0.20603, Precision = 50.81%, Recall = 66.89%, F1 = 57.76%\n",
            "Epoch 13 / 100, Epoch Time = 4.45s: Train Loss = 0.18914, Precision = 48.52%, Recall = 76.15%, F1 = 59.27%, Val Loss = 0.21181, Precision = 55.47%, Recall = 63.06%, F1 = 59.02%\n",
            "Epoch 14 / 100, Epoch Time = 4.38s: Train Loss = 0.18731, Precision = 49.24%, Recall = 75.72%, F1 = 59.67%, Val Loss = 0.20388, Precision = 41.93%, Recall = 72.62%, F1 = 53.16%\n",
            "[67 / 133]: Loss = 0.19132, Precision = 75.00%, Recall = 61.02%, F1 = 67.29%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15 / 100, Epoch Time = 4.37s: Train Loss = 0.18482, Precision = 50.09%, Recall = 76.79%, F1 = 60.63%, Val Loss = 0.21244, Precision = 34.27%, Recall = 77.26%, F1 = 47.48%\n",
            "Epoch 16 / 100, Epoch Time = 4.46s: Train Loss = 0.18089, Precision = 50.89%, Recall = 76.66%, F1 = 61.17%, Val Loss = 0.20305, Precision = 40.87%, Recall = 74.13%, F1 = 52.69%\n",
            "Epoch 17 / 100, Epoch Time = 4.39s: Train Loss = 0.17806, Precision = 52.15%, Recall = 78.15%, F1 = 62.56%, Val Loss = 0.19990, Precision = 50.01%, Recall = 68.63%, F1 = 57.86%\n",
            "Epoch 18 / 100, Epoch Time = 4.48s: Train Loss = 0.17455, Precision = 52.91%, Recall = 78.20%, F1 = 63.11%, Val Loss = 0.19854, Precision = 48.41%, Recall = 70.23%, F1 = 57.32%\n",
            "Epoch 19 / 100, Epoch Time = 4.50s: Train Loss = 0.17294, Precision = 53.54%, Recall = 77.84%, F1 = 63.44%, Val Loss = 0.19920, Precision = 50.67%, Recall = 69.23%, F1 = 58.51%\n",
            "Epoch 20 / 100, Epoch Time = 4.42s: Train Loss = 0.16860, Precision = 54.62%, Recall = 78.94%, F1 = 64.56%, Val Loss = 0.19998, Precision = 48.50%, Recall = 70.27%, F1 = 57.39%\n",
            "Epoch 21 / 100, Epoch Time = 4.35s: Train Loss = 0.16615, Precision = 55.29%, Recall = 79.35%, F1 = 65.17%, Val Loss = 0.20062, Precision = 55.70%, Recall = 65.91%, F1 = 60.38%\n",
            "[65 / 133]: Loss = 0.14127, Precision = 85.37%, Recall = 67.31%, F1 = 75.27%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22 / 100, Epoch Time = 4.45s: Train Loss = 0.16248, Precision = 56.59%, Recall = 79.75%, F1 = 66.20%, Val Loss = 0.20041, Precision = 55.82%, Recall = 66.14%, F1 = 60.54%\n",
            "Epoch 23 / 100, Epoch Time = 4.47s: Train Loss = 0.15967, Precision = 56.88%, Recall = 80.10%, F1 = 66.52%, Val Loss = 0.20658, Precision = 56.79%, Recall = 64.35%, F1 = 60.33%\n",
            "Epoch 24 / 100, Epoch Time = 4.36s: Train Loss = 0.15673, Precision = 58.45%, Recall = 80.62%, F1 = 67.77%, Val Loss = 0.20221, Precision = 50.10%, Recall = 69.87%, F1 = 58.36%\n",
            "Epoch 25 / 100, Epoch Time = 4.43s: Train Loss = 0.15281, Precision = 59.07%, Recall = 81.08%, F1 = 68.35%, Val Loss = 0.20099, Precision = 53.99%, Recall = 67.51%, F1 = 60.00%\n",
            "Epoch 26 / 100, Epoch Time = 4.35s: Train Loss = 0.14979, Precision = 59.92%, Recall = 81.41%, F1 = 69.03%, Val Loss = 0.20304, Precision = 47.56%, Recall = 71.57%, F1 = 57.14%\n",
            "Epoch 27 / 100, Epoch Time = 4.36s: Train Loss = 0.14717, Precision = 60.58%, Recall = 81.73%, F1 = 69.58%, Val Loss = 0.20490, Precision = 50.21%, Recall = 71.28%, F1 = 58.92%\n",
            "Epoch 28 / 100, Epoch Time = 4.51s: Train Loss = 0.14265, Precision = 61.87%, Recall = 82.92%, F1 = 70.86%, Val Loss = 0.20778, Precision = 55.62%, Recall = 66.51%, F1 = 60.58%\n",
            "[70 / 133]: Loss = 0.14647, Precision = 91.67%, Recall = 66.00%, F1 = 76.74%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29 / 100, Epoch Time = 4.39s: Train Loss = 0.13881, Precision = 62.40%, Recall = 83.36%, F1 = 71.38%, Val Loss = 0.21165, Precision = 51.16%, Recall = 69.19%, F1 = 58.82%\n",
            "Epoch 30 / 100, Epoch Time = 4.34s: Train Loss = 0.13620, Precision = 63.24%, Recall = 83.75%, F1 = 72.07%, Val Loss = 0.21207, Precision = 46.44%, Recall = 72.58%, F1 = 56.64%\n",
            "Epoch 31 / 100, Epoch Time = 4.42s: Train Loss = 0.13248, Precision = 64.29%, Recall = 83.91%, F1 = 72.81%, Val Loss = 0.21325, Precision = 50.41%, Recall = 70.22%, F1 = 58.69%\n",
            "Epoch 32 / 100, Epoch Time = 4.41s: Train Loss = 0.12859, Precision = 65.04%, Recall = 84.91%, F1 = 73.66%, Val Loss = 0.21561, Precision = 53.93%, Recall = 68.59%, F1 = 60.38%\n",
            "Epoch 33 / 100, Epoch Time = 4.37s: Train Loss = 0.12634, Precision = 65.72%, Recall = 85.12%, F1 = 74.18%, Val Loss = 0.21307, Precision = 53.02%, Recall = 69.48%, F1 = 60.14%\n",
            "Epoch 34 / 100, Epoch Time = 4.49s: Train Loss = 0.12369, Precision = 67.04%, Recall = 85.46%, F1 = 75.14%, Val Loss = 0.21735, Precision = 57.73%, Recall = 64.99%, F1 = 61.15%\n",
            "Epoch 35 / 100, Epoch Time = 4.35s: Train Loss = 0.11949, Precision = 67.53%, Recall = 85.33%, F1 = 75.39%, Val Loss = 0.22549, Precision = 51.04%, Recall = 70.18%, F1 = 59.10%\n",
            "[72 / 133]: Loss = 0.13297, Precision = 95.00%, Recall = 61.29%, F1 = 74.51%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36 / 100, Epoch Time = 4.40s: Train Loss = 0.11563, Precision = 68.60%, Recall = 86.02%, F1 = 76.32%, Val Loss = 0.22816, Precision = 50.50%, Recall = 70.23%, F1 = 58.75%\n",
            "Epoch 37 / 100, Epoch Time = 4.38s: Train Loss = 0.11281, Precision = 69.17%, Recall = 86.43%, F1 = 76.84%, Val Loss = 0.23123, Precision = 53.59%, Recall = 68.56%, F1 = 60.15%\n",
            "Epoch 38 / 100, Epoch Time = 4.37s: Train Loss = 0.10883, Precision = 70.89%, Recall = 86.88%, F1 = 78.08%, Val Loss = 0.23661, Precision = 49.44%, Recall = 69.76%, F1 = 57.87%\n",
            "Epoch 39 / 100, Epoch Time = 4.48s: Train Loss = 0.10466, Precision = 71.56%, Recall = 87.44%, F1 = 78.71%, Val Loss = 0.23766, Precision = 53.87%, Recall = 67.66%, F1 = 59.98%\n",
            "Epoch 40 / 100, Epoch Time = 4.36s: Train Loss = 0.10116, Precision = 72.63%, Recall = 87.10%, F1 = 79.21%, Val Loss = 0.25221, Precision = 49.36%, Recall = 70.61%, F1 = 58.10%\n",
            "Epoch 41 / 100, Epoch Time = 4.53s: Train Loss = 0.09859, Precision = 73.41%, Recall = 87.63%, F1 = 79.89%, Val Loss = 0.25847, Precision = 54.67%, Recall = 64.61%, F1 = 59.23%\n",
            "Epoch 42 / 100, Epoch Time = 4.49s: Train Loss = 0.09657, Precision = 73.50%, Recall = 88.35%, F1 = 80.24%, Val Loss = 0.25977, Precision = 54.30%, Recall = 66.18%, F1 = 59.65%\n",
            "[63 / 133]: Loss = 0.07636, Precision = 90.38%, Recall = 81.03%, F1 = 85.45%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43 / 100, Epoch Time = 4.49s: Train Loss = 0.09265, Precision = 74.63%, Recall = 88.48%, F1 = 80.97%, Val Loss = 0.26484, Precision = 55.93%, Recall = 64.16%, F1 = 59.76%\n",
            "Epoch 44 / 100, Epoch Time = 4.44s: Train Loss = 0.09074, Precision = 75.26%, Recall = 88.36%, F1 = 81.29%, Val Loss = 0.26849, Precision = 50.73%, Recall = 68.72%, F1 = 58.37%\n",
            "Epoch 45 / 100, Epoch Time = 4.53s: Train Loss = 0.08651, Precision = 76.50%, Recall = 88.70%, F1 = 82.15%, Val Loss = 0.26938, Precision = 52.07%, Recall = 67.91%, F1 = 58.95%\n",
            "Epoch 46 / 100, Epoch Time = 4.38s: Train Loss = 0.08346, Precision = 77.60%, Recall = 89.85%, F1 = 83.28%, Val Loss = 0.28628, Precision = 56.70%, Recall = 63.08%, F1 = 59.72%\n",
            "Epoch 47 / 100, Epoch Time = 4.43s: Train Loss = 0.08024, Precision = 78.75%, Recall = 89.85%, F1 = 83.94%, Val Loss = 0.28107, Precision = 54.99%, Recall = 66.23%, F1 = 60.09%\n",
            "Epoch 48 / 100, Epoch Time = 4.44s: Train Loss = 0.07963, Precision = 78.82%, Recall = 89.15%, F1 = 83.67%, Val Loss = 0.28748, Precision = 48.73%, Recall = 68.15%, F1 = 56.82%\n",
            "Epoch 49 / 100, Epoch Time = 4.43s: Train Loss = 0.07621, Precision = 79.44%, Recall = 90.46%, F1 = 84.59%, Val Loss = 0.29153, Precision = 54.56%, Recall = 63.49%, F1 = 58.68%\n",
            "[66 / 133]: Loss = 0.06280, Precision = 90.38%, Recall = 85.45%, F1 = 87.85%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50 / 100, Epoch Time = 4.47s: Train Loss = 0.07320, Precision = 81.08%, Recall = 90.11%, F1 = 85.36%, Val Loss = 0.31047, Precision = 54.70%, Recall = 63.23%, F1 = 58.66%\n",
            "Epoch 51 / 100, Epoch Time = 4.37s: Train Loss = 0.07113, Precision = 81.51%, Recall = 90.75%, F1 = 85.88%, Val Loss = 0.30517, Precision = 57.30%, Recall = 62.09%, F1 = 59.60%\n",
            "Epoch 52 / 100, Epoch Time = 4.38s: Train Loss = 0.06888, Precision = 81.71%, Recall = 90.55%, F1 = 85.90%, Val Loss = 0.31546, Precision = 55.93%, Recall = 65.04%, F1 = 60.14%\n",
            "Epoch 53 / 100, Epoch Time = 4.40s: Train Loss = 0.06550, Precision = 83.39%, Recall = 91.29%, F1 = 87.16%, Val Loss = 0.31726, Precision = 57.16%, Recall = 62.85%, F1 = 59.87%\n",
            "Epoch 54 / 100, Epoch Time = 4.43s: Train Loss = 0.06409, Precision = 83.77%, Recall = 91.59%, F1 = 87.50%, Val Loss = 0.32145, Precision = 54.64%, Recall = 63.80%, F1 = 58.87%\n",
            "Epoch 55 / 100, Epoch Time = 4.44s: Train Loss = 0.06149, Precision = 84.37%, Recall = 91.67%, F1 = 87.87%, Val Loss = 0.33426, Precision = 56.30%, Recall = 62.26%, F1 = 59.13%\n",
            "Epoch 56 / 100, Epoch Time = 4.47s: Train Loss = 0.06063, Precision = 84.87%, Recall = 92.04%, F1 = 88.31%, Val Loss = 0.33539, Precision = 54.67%, Recall = 61.73%, F1 = 57.99%\n",
            "[62 / 133]: Loss = 0.06612, Precision = 97.44%, Recall = 82.61%, F1 = 89.41%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 57 / 100, Epoch Time = 4.49s: Train Loss = 0.05821, Precision = 85.57%, Recall = 92.11%, F1 = 88.72%, Val Loss = 0.34338, Precision = 56.64%, Recall = 60.37%, F1 = 58.45%\n",
            "Epoch 58 / 100, Epoch Time = 4.38s: Train Loss = 0.05591, Precision = 86.41%, Recall = 92.67%, F1 = 89.43%, Val Loss = 0.35184, Precision = 57.53%, Recall = 59.75%, F1 = 58.62%\n",
            "Epoch 59 / 100, Epoch Time = 4.39s: Train Loss = 0.05423, Precision = 86.75%, Recall = 92.64%, F1 = 89.60%, Val Loss = 0.36212, Precision = 55.93%, Recall = 63.21%, F1 = 59.35%\n",
            "Epoch 60 / 100, Epoch Time = 4.44s: Train Loss = 0.05161, Precision = 87.73%, Recall = 93.12%, F1 = 90.34%, Val Loss = 0.37065, Precision = 54.84%, Recall = 60.88%, F1 = 57.71%\n",
            "Epoch 61 / 100, Epoch Time = 4.40s: Train Loss = 0.05049, Precision = 88.25%, Recall = 92.88%, F1 = 90.50%, Val Loss = 0.37858, Precision = 52.70%, Recall = 63.45%, F1 = 57.58%\n",
            "Epoch 62 / 100, Epoch Time = 4.41s: Train Loss = 0.05110, Precision = 87.53%, Recall = 92.78%, F1 = 90.08%, Val Loss = 0.38512, Precision = 61.07%, Recall = 56.33%, F1 = 58.60%\n",
            "Epoch 63 / 100, Epoch Time = 4.47s: Train Loss = 0.04968, Precision = 88.21%, Recall = 92.82%, F1 = 90.45%, Val Loss = 0.37883, Precision = 55.64%, Recall = 60.28%, F1 = 57.87%\n",
            "[72 / 133]: Loss = 0.05304, Precision = 96.67%, Recall = 87.88%, F1 = 92.06%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 64 / 100, Epoch Time = 4.43s: Train Loss = 0.04500, Precision = 89.49%, Recall = 93.98%, F1 = 91.68%, Val Loss = 0.39405, Precision = 55.53%, Recall = 59.80%, F1 = 57.59%\n",
            "Epoch 65 / 100, Epoch Time = 4.45s: Train Loss = 0.04588, Precision = 89.35%, Recall = 93.37%, F1 = 91.32%, Val Loss = 0.38897, Precision = 55.47%, Recall = 59.98%, F1 = 57.64%\n",
            "Epoch 66 / 100, Epoch Time = 4.45s: Train Loss = 0.04512, Precision = 89.51%, Recall = 93.38%, F1 = 91.40%, Val Loss = 0.40387, Precision = 49.30%, Recall = 64.73%, F1 = 55.97%\n",
            "Epoch 67 / 100, Epoch Time = 4.43s: Train Loss = 0.04284, Precision = 90.03%, Recall = 94.11%, F1 = 92.02%, Val Loss = 0.40470, Precision = 55.87%, Recall = 59.90%, F1 = 57.81%\n",
            "Epoch 68 / 100, Epoch Time = 4.36s: Train Loss = 0.04209, Precision = 90.24%, Recall = 93.78%, F1 = 91.98%, Val Loss = 0.42783, Precision = 52.70%, Recall = 60.62%, F1 = 56.38%\n",
            "Epoch 69 / 100, Epoch Time = 4.52s: Train Loss = 0.03976, Precision = 91.08%, Recall = 94.52%, F1 = 92.77%, Val Loss = 0.42133, Precision = 55.24%, Recall = 60.41%, F1 = 57.71%\n",
            "Epoch 70 / 100, Epoch Time = 4.40s: Train Loss = 0.03774, Precision = 91.33%, Recall = 94.73%, F1 = 93.00%, Val Loss = 0.42297, Precision = 56.13%, Recall = 60.26%, F1 = 58.12%\n",
            "[68 / 133]: Loss = 0.03536, Precision = 98.44%, Recall = 90.00%, F1 = 94.03%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 71 / 100, Epoch Time = 4.37s: Train Loss = 0.03833, Precision = 91.30%, Recall = 94.23%, F1 = 92.74%, Val Loss = 0.43380, Precision = 55.70%, Recall = 58.16%, F1 = 56.91%\n",
            "Epoch 72 / 100, Epoch Time = 4.46s: Train Loss = 0.03619, Precision = 91.92%, Recall = 94.61%, F1 = 93.24%, Val Loss = 0.44784, Precision = 58.02%, Recall = 57.10%, F1 = 57.56%\n",
            "Epoch 73 / 100, Epoch Time = 4.38s: Train Loss = 0.03635, Precision = 91.88%, Recall = 94.40%, F1 = 93.12%, Val Loss = 0.43936, Precision = 57.62%, Recall = 58.96%, F1 = 58.28%\n",
            "Epoch 74 / 100, Epoch Time = 4.39s: Train Loss = 0.03666, Precision = 92.04%, Recall = 94.83%, F1 = 93.42%, Val Loss = 0.45530, Precision = 55.87%, Recall = 59.31%, F1 = 57.54%\n",
            "Epoch 75 / 100, Epoch Time = 4.36s: Train Loss = 0.03732, Precision = 91.33%, Recall = 94.37%, F1 = 92.82%, Val Loss = 0.45984, Precision = 53.70%, Recall = 57.76%, F1 = 55.66%\n",
            "Epoch 76 / 100, Epoch Time = 4.33s: Train Loss = 0.03399, Precision = 92.49%, Recall = 95.13%, F1 = 93.79%, Val Loss = 0.45946, Precision = 56.53%, Recall = 58.26%, F1 = 57.38%\n",
            "Epoch 77 / 100, Epoch Time = 4.52s: Train Loss = 0.03384, Precision = 92.79%, Recall = 94.80%, F1 = 93.78%, Val Loss = 0.45877, Precision = 57.85%, Recall = 56.63%, F1 = 57.23%\n",
            "[73 / 133]: Loss = 0.06992, Precision = 90.74%, Recall = 85.96%, F1 = 88.29%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 78 / 100, Epoch Time = 4.36s: Train Loss = 0.03609, Precision = 92.04%, Recall = 94.47%, F1 = 93.24%, Val Loss = 0.47160, Precision = 58.53%, Recall = 56.72%, F1 = 57.61%\n",
            "Epoch 79 / 100, Epoch Time = 4.37s: Train Loss = 0.03272, Precision = 92.83%, Recall = 94.98%, F1 = 93.89%, Val Loss = 0.47623, Precision = 56.64%, Recall = 57.80%, F1 = 57.22%\n",
            "Epoch 80 / 100, Epoch Time = 4.34s: Train Loss = 0.02964, Precision = 93.58%, Recall = 95.43%, F1 = 94.50%, Val Loss = 0.49237, Precision = 55.39%, Recall = 59.38%, F1 = 57.31%\n",
            "Epoch 81 / 100, Epoch Time = 4.40s: Train Loss = 0.02931, Precision = 93.81%, Recall = 95.53%, F1 = 94.66%, Val Loss = 0.49424, Precision = 53.42%, Recall = 59.52%, F1 = 56.30%\n",
            "Epoch 82 / 100, Epoch Time = 4.38s: Train Loss = 0.02963, Precision = 93.50%, Recall = 95.74%, F1 = 94.61%, Val Loss = 0.48725, Precision = 53.87%, Recall = 59.20%, F1 = 56.41%\n",
            "Epoch 83 / 100, Epoch Time = 4.40s: Train Loss = 0.02782, Precision = 93.88%, Recall = 95.61%, F1 = 94.73%, Val Loss = 0.49223, Precision = 55.13%, Recall = 58.92%, F1 = 56.96%\n",
            "Epoch 84 / 100, Epoch Time = 4.35s: Train Loss = 0.02744, Precision = 94.31%, Recall = 95.73%, F1 = 95.02%, Val Loss = 0.49394, Precision = 55.64%, Recall = 58.36%, F1 = 56.97%\n",
            "[65 / 133]: Loss = 0.03742, Precision = 95.56%, Recall = 87.76%, F1 = 91.49%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 85 / 100, Epoch Time = 4.43s: Train Loss = 0.02688, Precision = 94.34%, Recall = 95.67%, F1 = 95.00%, Val Loss = 0.50564, Precision = 55.50%, Recall = 57.85%, F1 = 56.65%\n",
            "Epoch 86 / 100, Epoch Time = 4.34s: Train Loss = 0.02617, Precision = 94.51%, Recall = 95.88%, F1 = 95.19%, Val Loss = 0.51153, Precision = 53.42%, Recall = 59.58%, F1 = 56.33%\n",
            "Epoch 87 / 100, Epoch Time = 4.35s: Train Loss = 0.02484, Precision = 94.76%, Recall = 96.09%, F1 = 95.42%, Val Loss = 0.51215, Precision = 54.47%, Recall = 59.58%, F1 = 56.91%\n",
            "Epoch 88 / 100, Epoch Time = 4.42s: Train Loss = 0.02624, Precision = 94.12%, Recall = 95.71%, F1 = 94.91%, Val Loss = 0.51644, Precision = 56.04%, Recall = 58.64%, F1 = 57.31%\n",
            "Epoch 89 / 100, Epoch Time = 4.42s: Train Loss = 0.02566, Precision = 94.35%, Recall = 95.95%, F1 = 95.15%, Val Loss = 0.51726, Precision = 57.90%, Recall = 57.44%, F1 = 57.67%\n",
            "Epoch 90 / 100, Epoch Time = 4.35s: Train Loss = 0.02435, Precision = 94.83%, Recall = 95.91%, F1 = 95.37%, Val Loss = 0.53171, Precision = 53.16%, Recall = 59.65%, F1 = 56.22%\n",
            "Epoch 91 / 100, Epoch Time = 4.47s: Train Loss = 0.02406, Precision = 94.93%, Recall = 95.91%, F1 = 95.42%, Val Loss = 0.51832, Precision = 54.27%, Recall = 59.38%, F1 = 56.71%\n",
            "[66 / 133]: Loss = 0.01463, Precision = 100.00%, Recall = 98.18%, F1 = 99.08%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 92 / 100, Epoch Time = 4.36s: Train Loss = 0.02589, Precision = 94.31%, Recall = 95.87%, F1 = 95.09%, Val Loss = 0.54971, Precision = 51.87%, Recall = 59.22%, F1 = 55.30%\n",
            "Epoch 93 / 100, Epoch Time = 4.49s: Train Loss = 0.02553, Precision = 94.76%, Recall = 95.59%, F1 = 95.17%, Val Loss = 0.56187, Precision = 51.73%, Recall = 59.50%, F1 = 55.34%\n",
            "Epoch 94 / 100, Epoch Time = 4.35s: Train Loss = 0.02454, Precision = 94.55%, Recall = 95.95%, F1 = 95.24%, Val Loss = 0.53982, Precision = 55.33%, Recall = 59.04%, F1 = 57.13%\n",
            "Epoch 95 / 100, Epoch Time = 4.55s: Train Loss = 0.02294, Precision = 95.06%, Recall = 96.08%, F1 = 95.56%, Val Loss = 0.55452, Precision = 54.82%, Recall = 58.58%, F1 = 56.64%\n",
            "Epoch 96 / 100, Epoch Time = 4.51s: Train Loss = 0.02181, Precision = 95.41%, Recall = 96.28%, F1 = 95.84%, Val Loss = 0.56053, Precision = 53.53%, Recall = 58.66%, F1 = 55.98%\n",
            "Epoch 97 / 100, Epoch Time = 4.34s: Train Loss = 0.02035, Precision = 95.88%, Recall = 96.33%, F1 = 96.10%, Val Loss = 0.55560, Precision = 51.96%, Recall = 61.07%, F1 = 56.15%\n",
            "Epoch 98 / 100, Epoch Time = 4.34s: Train Loss = 0.02116, Precision = 95.43%, Recall = 96.19%, F1 = 95.81%, Val Loss = 0.55971, Precision = 57.87%, Recall = 56.33%, F1 = 57.09%\n",
            "[69 / 133]: Loss = 0.01088, Precision = 97.67%, Recall = 97.67%, F1 = 97.67%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 99 / 100, Epoch Time = 4.37s: Train Loss = 0.02229, Precision = 95.13%, Recall = 96.38%, F1 = 95.75%, Val Loss = 0.55678, Precision = 56.90%, Recall = 55.03%, F1 = 55.95%\n",
            "Epoch 100 / 100, Epoch Time = 4.38s: Train Loss = 0.02608, Precision = 94.41%, Recall = 95.64%, F1 = 95.02%, Val Loss = 0.54796, Precision = 56.42%, Recall = 56.72%, F1 = 56.57%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gz1Aw4rnITMS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LSTM и CNN сравнимы по качеству и скорости обучения."
      ]
    },
    {
      "metadata": {
        "id": "xOt1tcOkvDBY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Визуализация полученных свёрток"
      ]
    },
    {
      "metadata": {
        "id": "6NO_aVBDvKcT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Мы обучили набор свёрток и эмбеддинги. Давайте посмотрим, на какие именно символы загораются свёртки."
      ]
    },
    {
      "metadata": {
        "id": "DA0ndcTJ0O-s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filters = next(model.cnn.parameters())\n",
        "embeddings = next(model.char_emb.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqutnmzWvdUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Рассмотрим только маленькие буквы:"
      ]
    },
    {
      "metadata": {
        "id": "sSAIKGCcF156",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_range(first_symb, last_symb):\n",
        "    return [chr(c) for c in range(ord(first_symb), ord(last_symb) + 1)]\n",
        "  \n",
        "russian_letters = [''] + get_range('а', 'я')\n",
        "russian_letters_idx = [char_index[letter] for letter in russian_letters]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0b7Fwp2vpT7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Эмбеддинг триграммы - это просто конкатенация эмбеддингов её символов:"
      ]
    },
    {
      "metadata": {
        "id": "8b8h1MOFAzhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "suffix = 'сев'\n",
        "\n",
        "suffix_embedding = torch.cat([embeddings[char_index[letter]] for letter in suffix] + \n",
        "                             [embeddings[char_index['']] for i in range(3 - len(suffix))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbJ8IVlrv5oh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посчитайте, какой из фильтров сильнее всего реагирует на триграмму."
      ]
    },
    {
      "metadata": {
        "id": "M0wdpX3EwJxA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "6131f858-34bf-4698-84ee-d4b833755622"
      },
      "cell_type": "code",
      "source": [
        "filters = filters.squeeze()\n",
        "\n",
        "results = np.zeros(filters.size()[0])\n",
        "for i in range(len(results)):\n",
        "  results[i] = filters[i].view(-1) @ suffix_embedding\n",
        "best_filter = filters[results.argmax()]\n",
        "print(best_filter)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable containing:\n",
            "\n",
            "Columns 0 to 9 \n",
            "-0.3278 -0.0518  0.2497 -0.2013 -0.5525  0.1958 -0.3295  0.1783  0.2111  0.1088\n",
            " 0.3377 -0.3085  0.3372  0.2955 -0.2675 -0.3284 -0.1611 -0.0438 -0.0457  0.0491\n",
            "-0.1486 -0.1545 -0.0296 -0.1378 -0.4260 -0.0602  0.1174  0.0485  0.2953 -0.2982\n",
            "\n",
            "Columns 10 to 19 \n",
            " 0.2604 -0.0131  0.1360  0.0634  0.3940 -0.1501 -0.1620 -0.1147 -0.0482 -0.0141\n",
            "-0.0294  0.2876  0.1755 -0.0593  0.1115  0.0339 -0.2873 -0.2326 -0.0204 -0.3649\n",
            " 0.3713  0.2002 -0.1125 -0.0289 -0.2904  0.0066  0.3135 -0.0051 -0.0564 -0.1105\n",
            "\n",
            "Columns 20 to 23 \n",
            " 0.1294 -0.0597  0.2798 -0.1151\n",
            " 0.1583 -0.0912 -0.0277 -0.0975\n",
            "-0.1339  0.1162  0.1478  0.1844\n",
            "[torch.cuda.FloatTensor of size 3x24 (GPU 0)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bmtp3FDEwOwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А теперь - наоборот, подсчитаем, как найденный фильтр реагирует на все символы из `russian_letters`.\n",
        "\n",
        "Нужно построить матрицу `sim` размера `3 x len(russian_letters)`, в каждом элементе которой будет записано, насколько сильно данный элемент фильтра реагирует на данный символ."
      ]
    },
    {
      "metadata": {
        "id": "8oT2ml0F2gVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "f8ae858b-b866-45dd-c295-c0657118d9eb"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "sim = np.zeros((3, len(russian_letters)))\n",
        "\n",
        "for i in range(3):\n",
        "  for j in range(len(russian_letters)):\n",
        "    sim[i, j] = best_filter[i] @ embeddings[russian_letters_idx[j]]\n",
        "\n",
        "fig = plt.figure(figsize=(30, 5))\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(sim, cmap='bone')\n",
        "fig.colorbar(cax)\n",
        "\n",
        "ax.set_xticklabels([''] + russian_letters)\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABcoAAAEeCAYAAAC66iZnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngLH2/wHKQs61gxHPiwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fadd8e1bc18>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JXWmMkWMfUYZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention and Convolutions"
      ]
    },
    {
      "metadata": {
        "id": "1WXv4ObuZUCN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напомню, что attention работает так: пусть у нас есть набор скрытых состояний $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m$ - например, представлений слов из исходного языка, полученных с помощью энкодера. И есть некоторое текущее скрытое состояние $\\mathbf{h}_i$ - скажем, представление, используемое для предсказания слова на нужном нам языке.\n",
        "\n",
        "Тогда с помощью аттеншена мы можем получить взвешенное представление контекста $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m$ - вектор $c_i$:\n",
        "$$\n",
        "\\begin{align}\\begin{split}\n",
        "\\mathbf{c}_i &= \\sum\\limits_j a_{ij}\\mathbf{s}_j\\\\\n",
        "\\mathbf{a}_{ij} &= \\text{softmax}(f_{att}(\\mathbf{h}_i, \\mathbf{s}_j))\n",
        "\\end{split}\\end{align}\n",
        "$$\n",
        "\n",
        "$f_{att}$ - функция, которая говорит, насколько хорошо $\\mathbf{h}_i$ и $\\mathbf{s}_j$ подходят друг другу.\n",
        "\n",
        "Самые популярные её варианты:\n",
        "- Additive attention:\n",
        "$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{v}_a{}^\\top \\text{tanh}(\\mathbf{W}_a[\\mathbf{h}_i; \\mathbf{s}_j])$$\n",
        "- Dot attention:\n",
        "$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{h}_i^\\top \\mathbf{s}_j$$\n",
        "- Multiplicative attention:\n",
        "$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{h}_i^\\top \\mathbf{W}_a \\mathbf{s}_j$$\n",
        "\n",
        "Есть ещё одна вариация на тему - self-attention. Это когда у нас нет $\\mathbf{h}_i$ - только $\\mathbf{s}_j$-тые, т.е. вектора-представления исходной последовательности. Такое может очень естественно возникнуть практически в любой задаче - например, в классификации текстов.\n",
        "\n",
        "Additive self-attention можно записать как $f_{att}(\\mathbf{s}_i) = \\mathbf{v}_a{}^\\top \\text{tanh}(\\mathbf{W}_a \\mathbf{s}_i)$. Тогда по последовательности $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m = \\mathbf{S}$ вычисляется единственный вектор:\n",
        "$$\n",
        "\\begin{align}\\begin{split}\n",
        "\\mathbf{a} &= \\text{softmax}(\\mathbf{v}_a \\text{tanh}(\\mathbf{W}_a \\mathbf{S}^\\top))\\\\\n",
        "\\mathbf{c} & = \\mathbf{S} \\mathbf{a}^\\top\n",
        "\\end{split}\\end{align}\n",
        "$$\n",
        "\n",
        "При чём здесь были упомянуты свёртки? В основном при том, что они, вообще говоря, действуют похожим образом. \n",
        "\n",
        "Давайте посмотрим на операцию $\\mathbf{v}_a \\text{tanh}(\\mathbf{W}_a \\mathbf{S}^\\top)$ как на извращенный вариант свертки с размером фильтра 1. Для каждого вектора $\\mathbf{s_j}$ строится некоторая оценка того, насколько важно данное состояние.\n",
        "\n",
        "Кроме того, кроме max-pooling'а существует ещё и avg-pooling - когда берется не максимальный сигнал, полученный сверткой, а средний. В какой-то степени $\\mathbf{c} = \\mathbf{S} \\mathbf{a}^\\top$ - тоже усреднение. Правда, усредняются исходные вектора, а не получающиеся оценки.\n",
        "\n",
        "В таком self-attention мы выучиваем только один аналог фильтра сверточной сети. Но в сверточных сетях же много фильтров. Давайте тогда учить сразу несколько вариантов attention'а:\n",
        "$$\n",
        "\\begin{align}\\begin{split}\n",
        "\\mathbf{A} &= \\text{softmax}(\\mathbf{V}_a \\text{tanh}(\\mathbf{W}_a \\mathbf{H}^\\top))\\\\\n",
        "\\mathbf{C} & = \\mathbf{A} \\mathbf{H}\n",
        "\\end{split}\\end{align}\n",
        "$$\n",
        "\n",
        "Утверждается, что случайной инициализации вполне достаточно, чтобы attention начал отлавливать разные характеристики из последовательности - такие лексическое и грамматическое значение слов. Но это и вполне естественно, если посмотреть, что свертки уже много лет так учат."
      ]
    },
    {
      "metadata": {
        "id": "jQ8WR867xprA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention Is All You Need\n",
        "\n",
        "В середине прошлого года вышла статья [Attention Is All You Need](https://arxiv.org/abs/1706.03762), которая опирается на эти идеи.\n",
        "\n",
        "Она описывает Transformer - архитектуру полносвязной сети, которая делает то, что раньше все делали с помощью рекуррентных сетей. Хороший обзор был на хабре: [Transformer — новая архитектура нейросетей для работы с последовательностями](https://habrahabr.ru/post/341240/).\n",
        "\n",
        "К задаче машинного перевода она применима так:  \n",
        "![transformer](https://hsto.org/webt/59/f0/44/59f04410c0e56192990801.png =x600)\n",
        "\n",
        "Из интересного - multi-head attention, который является аналогом того, что мы рассматривали в предыдущем разделе:  \n",
        "![multi-head attn](https://hsto.org/webt/59/f0/44/59f0440f1109b864893781.png)\n",
        "\n",
        "Сам attention выглядит так: \n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V,$$\n",
        "\n",
        "Выглядит страшно, но в реальности передают в качестве $Q, K, V$ одну и ту же последовательность скрытых состояний - $\\mathbf{S}$ из предыдущего раздела.\n",
        "\n",
        "$Q K^\\top$ - это как dot-attention.\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^o$$ \n",
        "$$\\text{head}_i = \\text{Attention}(Q W^Q_i, K W^K_i, V W^V_i).$$\n",
        "\n",
        "На pytorch это выглядит так (утащено из [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch) и [seq2seq.pytorch](https://github.com/eladhoffer/seq2seq.pytorch)):"
      ]
    },
    {
      "metadata": {
        "id": "ikj6l4vofT7Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dropout=0, causal=False):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        b_q, t_q, dim_q = q.size()\n",
        "        b_k, t_k, dim_k = k.size()\n",
        "        b_v, t_v, dim_v = v.size()\n",
        "        qk = torch.bmm(q, k.transpose(1, 2))  # b x t_q x t_k\n",
        "        qk.div_(dim_k ** 0.5)\n",
        "        sm_qk = F.softmax(qk, dim=2)\n",
        "        sm_qk = self.dropout(sm_qk)\n",
        "        return torch.bmm(sm_qk, v), sm_qk  # b x t_q x dim_v\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, num_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert(input_size % num_heads == 0)\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        self.linear_q = nn.Linear(input_size, input_size)\n",
        "        self.linear_k = nn.Linear(input_size, input_size)\n",
        "        self.linear_v = nn.Linear(input_size, input_size)\n",
        "        self.linear_out = nn.Linear(input_size, output_size)\n",
        "        \n",
        "        self.sdp_attention = ScaledDotProductAttention(dropout=dropout)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        b_q, t_q, dim_q = q.size()\n",
        "        b_k, t_k, dim_k = k.size()\n",
        "        b_v, t_v, dim_v = v.size()\n",
        "        \n",
        "        qw = self.linear_q(q)\n",
        "        kw = self.linear_k(k)\n",
        "        vw = self.linear_v(v)\n",
        "        \n",
        "        qw = qw.chunk(self.num_heads, 2)\n",
        "        kw = kw.chunk(self.num_heads, 2)\n",
        "        vw = vw.chunk(self.num_heads, 2)\n",
        "        \n",
        "        output = []\n",
        "        attention_scores = []\n",
        "        for i in range(self.num_heads):\n",
        "            out_h, score = self.sdp_attention(qw[i], kw[i], vw[i])\n",
        "            output.append(out_h)\n",
        "            attention_scores.append(score)\n",
        "\n",
        "        output = torch.cat(output, 2)\n",
        "\n",
        "        return self.linear_out(output), attention_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0la3q01F3Au2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Чтобы всё это заработало, нужно смотреть не только на эмбеддинги слов, но и на их позиции в тексте. Предлагается использовать позиционное кодирование вида:\n",
        "$$\\text{PE}_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})$$\n",
        "$$\\text{PE}_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})$$\n",
        "\n",
        "Хотя можно и просто учить эмбеддинги.\n",
        "\n",
        "Их можно предподсчитать:"
      ]
    },
    {
      "metadata": {
        "id": "7epSQsRb3i_3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def position_encoding_init(n_position, d_pos_vec):\n",
        "    ''' Init the sinusoid position encoding table '''\n",
        "\n",
        "    # keep dim 0 for padding token position encoding zero vector\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
        "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
        "\n",
        "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
        "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
        "    return torch.from_numpy(position_enc).type(FloatTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eTeCJDXxTdp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Энкодер выглядит просто:"
      ]
    },
    {
      "metadata": {
        "id": "uUcF9_nKxZzp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=512, num_heads=8, inner_linear=1024, layer_norm=True, dropout=0):\n",
        "\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        if layer_norm:\n",
        "            self.lnorm1 = LayerNorm1d(hidden_size)\n",
        "            self.lnorm2 = LayerNorm1d(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.attention = MultiHeadAttention(hidden_size, hidden_size, num_heads, dropout=dropout)\n",
        "        self.fc = nn.Sequential(nn.Linear(hidden_size, inner_linear),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Dropout(dropout),\n",
        "                                nn.Linear(inner_linear, hidden_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        x, _ = self.attention(x, x, x)\n",
        "        x = self.dropout(x).add_(res)\n",
        "        x = self.lnorm1(x) if hasattr(self, 'lnorm1') else x\n",
        "        res = x\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x).add_(res)\n",
        "        x = self.lnorm2(x) if hasattr(self, 'lnorm2') else x\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHKbLtMvyFH8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Декодер отличается только необходимостью добавить маскинг - он должен смотреть только на предыдущие сгенерированные состояния, но не на следующие, и attention'ом на выход энкодера."
      ]
    }
  ]
}